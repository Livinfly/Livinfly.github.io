<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='MIT6.5940 的学习笔记，QML 部分咕咕咕 TBD.'><title>『学习笔记』MIT6.5940-TinyML-and-Efficient-Deep-Learning-Computing | Livinfly's Blog</title><link rel=canonical href=https://livinfly.github.io/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/><link rel=stylesheet href=/scss/style.min.2fa48f0dd0f0d33ca7625fe6827d8e10fb2960632d3596baf9186cede4604f55.css><meta property='og:title' content="『学习笔记』MIT6.5940-TinyML-and-Efficient-Deep-Learning-Computing"><meta property='og:description' content="MIT6.5940 的学习笔记，QML 部分咕咕咕 TBD."><meta property='og:url' content='https://livinfly.github.io/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/'><meta property='og:site_name' content="Livinfly's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='学习笔记'><meta property='article:tag' content='MLsys'><meta property='article:published_time' content='2025-06-14T10:17:32+00:00'><meta property='article:modified_time' content='2025-06-14T18:55:17+08:00'><meta property='og:image' content='https://livinfly.github.io/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/cover.jpg'><meta name=twitter:site content="@Mengmm_JhaiL"><meta name=twitter:creator content="@Mengmm_JhaiL"><meta name=twitter:title content="『学习笔记』MIT6.5940-TinyML-and-Efficient-Deep-Learning-Computing"><meta name=twitter:description content="MIT6.5940 的学习笔记，QML 部分咕咕咕 TBD."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://livinfly.github.io/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/cover.jpg'><link rel="shortcut icon" href=/favicon.png><link rel=apple-touch-icon href=/favicon.png><script async src="https://www.googletagmanager.com/gtag/js?id=G-LKX43Y8KEL"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LKX43Y8KEL")}</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_6155a7461c712b4f.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🤣</span></figure><div class=site-meta><h1 class=site-name><a href=/>Livinfly's Blog</a></h1><h2 class=site-description>想要留下点温暖的地方</h2></div></header><ol class=menu-social><li><a href=mailto:luojie3m@163.com target=_blank title=Email rel=me><svg class="icon icon-tabler icon-tabler-mail" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="3" y="5" width="18" height="14" rx="2"/><polyline points="3 7 12 13 21 7"/></svg></a></li><li><a href=https://github.com/Livinfly target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/Mengmm_JhaiL target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>友链</span></a></li><li><a href=/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#lec01-引入>Lec01 引入</a></li><li><a href=#lec02-基础>Lec02 基础</a></li><li><a href=#lab0>Lab0</a></li><li><a href=#lec03---04-剪枝>Lec03 - 04 剪枝</a></li><li><a href=#lab1>Lab1</a></li><li><a href=#lec05-量化>Lec05 量化</a></li><li><a href=#lec06-量化提高结果>Lec06 量化提高结果</a><ol><li><a href=#post-training-quantization>Post-training Quantization</a></li><li><a href=#quantization-aware-training-qat>Quantization-Aware Training (QAT)</a></li><li><a href=#binaryternary-quantization>Binary/Ternary Quantization</a></li><li><a href=#mixed-precision-quantization>Mixed-Precision Quantization</a></li></ol></li><li><a href=#lab2>Lab2</a></li><li><a href=#lec07-nas-神经网络结构搜索>Lec07 NAS 神经网络结构搜索</a></li><li><a href=#lec08-nas-更高效>Lec08 NAS 更高效</a></li><li><a href=#lec09-知识蒸馏-kd>Lec09 知识蒸馏 KD</a><ol><li><a href=#匹配对齐什么>匹配/对齐什么？</a></li><li><a href=#online-distillation-在线蒸馏>online distillation 在线蒸馏</a><ol><li><a href=#self-distillation>self-distillation</a></li><li><a href=#deep-mutual-learning-互学习-dml>Deep Mutual Learning 互学习 DML</a></li></ol></li></ol></li><li><a href=#lec10-mcunet-tinyml>Lec10 MCUnet TinyML</a><ol><li><a href=#瓶颈>瓶颈</a></li><li><a href=#tinynas>TinyNAS</a><ol><li><a href=#patch-based-inference-分块>Patch-based Inference 分块</a></li><li><a href=#应用>应用</a></li></ol></li></ol></li><li><a href=#lec11-tinyengine>Lec11 TinyEngine</a><ol><li><a href=#loop-optimization-循环优化>Loop optimization 循环优化</a><ol><li><a href=#loop-reordering-循环重排>Loop reordering 循环重排</a></li><li><a href=#loop-tiling-循环分块>Loop tiling 循环分块</a></li></ol></li><li><a href=#loop-unrolling-循环展开>Loop unrolling 循环展开</a></li><li><a href=#simd-single-instruction-multiple-data-programming-单指令多数据>SIMD (single instruction, multiple data) programming 单指令多数据</a><ol><li><a href=#isa-instruction-set-architecture-指令集架构>ISA (Instruction set architecture) 指令集架构</a></li></ol></li><li><a href=#multithreading-多线程>Multithreading 多线程</a><ol><li><a href=#openmp>OpenMP</a></li><li><a href=#cuda>CUDA</a></li></ol></li><li><a href=#inference-optimization>Inference Optimization</a><ol><li><a href=#image-to-column-im2col-convolution>Image to Column (Im2col) convolution</a></li><li><a href=#in-place-depth-wise-convolution>In-place depth-wise convolution</a></li><li><a href=#nhwc-for-point-wise-convolution-nchw-for-depth-wise-convolution>NHWC for point-wise convolution, NCHW for depth-wise convolution</a></li><li><a href=#winograd-convolution>Winograd convolution</a></li></ol></li></ol></li><li><a href=#lec12-transfomer--llm>Lec12 Transfomer & LLM</a><ol><li><a href=#transformer-基础>Transformer 基础</a></li><li><a href=#transfomer-design-variants-变体>Transfomer Design Variants 变体</a><ol><li><a href=#encoder-decoder-t5>Encoder-Decoder (T5)</a></li><li><a href=#encoder-only-bert-bidirectional-encoder-representations-from-transformers>Encoder-only (BERT, Bidirectional Encoder Representations from Transformers)</a></li><li><a href=#decoder-only-gpt-generative-pre-trained-transformer>Decoder-only (GPT, Generative Pre-trained Transformer)</a></li></ol></li><li><a href=#absoluterelative-positional-encoding>Absolute/Relative Positional Encoding</a></li><li><a href=#kv-cache-optimization>KV cache optimization</a><ol><li><a href=#multi-head-attention-mha>Multi-Head Attention (MHA)</a></li><li><a href=#multi-query-attention-mqa>Multi-Query Attention (MQA)</a></li><li><a href=#grouped-query-attention-gqa>Grouped-Query Attention (GQA)</a></li></ol></li><li><a href=#ffn--swiglu-gated-linear-units>FFN => SwiGLU (Gated Linear Units)</a></li><li><a href=#llm>LLM</a></li></ol></li><li><a href=#lec13-llm-deployment-techniques>Lec13 LLM Deployment Techniques</a><ol><li><a href=#quantization>Quantization</a><ol><li><a href=#weight-activation-quantization-smoothquant>Weight-Activation Quantization: SmoothQuant</a></li><li><a href=#weight-only-quantization-awq-and-tinychat>Weight-Only Quantization: AWQ and TinyChat</a></li><li><a href=#qserve-w4a8kv4>QServe (W4A8KV4)</a></li></ol></li><li><a href=#pruning--sparsity>Pruning & Sparsity</a><ol><li><a href=#weight-sparsity-wanda>Weight Sparsity: Wanda</a></li><li><a href=#contextual-sparsity>Contextual Sparsity</a></li><li><a href=#attention-sparsity>Attention Sparsity</a></li></ol></li><li><a href=#llm-serving-systems>LLM Serving Systems</a><ol><li><a href=#important-metrics-指标-for-llm-serving>Important Metrics 指标 for LLM Serving</a></li><li><a href=#优化目标>优化目标</a></li><li><a href=#paged-attention-vllm>Paged Attention (vLLM)</a></li><li><a href=#flashattention>FlashAttention</a></li><li><a href=#speculative-decoding-推测性解码>Speculative Decoding 推测性解码</a></li><li><a href=#batching>Batching</a></li></ol></li></ol></li><li><a href=#lec-14-llm-post-training>Lec 14 LLM Post-Training</a><ol><li><a href=#llm-fine-tuning-微调>LLM Fine-Tuning 微调</a><ol><li><a href=#supervised-fine-tuning-sft-监督微调>Supervised Fine-Tuning (SFT) 监督微调</a></li><li><a href=#reinforcement-learning-from-human-feedback-rlhf-基于人类反馈的强化学习>Reinforcement Learning from Human Feedback (RLHF) 基于人类反馈的强化学习</a></li><li><a href=#parameter-efficient-fine-tuning-peft>Parameter Efficient Fine-Tuning (PEFT)</a></li></ol></li><li><a href=#multi-model-llms>Multi-model LLMs</a><ol><li><a href=#cross-attention-based-flamingo>Cross-Attention Based: Flamingo</a></li><li><a href=#visual-tokens-as-input-palm-e-vila>Visual Tokens as Input: PaLM-E, VILA</a></li><li><a href=#enabling-visual-outputs-vila-u>Enabling Visual Outputs: VILA-U</a></li></ol></li><li><a href=#prompt-engineering>Prompt Engineering</a><ol><li><a href=#in-context-learning-icl>In-Context Learning (ICL)</a></li><li><a href=#chain-of-thought-cot>Chain-of-Thought (CoT)</a></li><li><a href=#retrieval-augmented-generation-rag>ReTrieval Augmented Generation (RAG)</a></li></ol></li></ol></li><li><a href=#lec15-long-context-llm>Lec15 Long-Context LLM</a><ol><li><a href=#context-extension>Context Extension</a><ol><li><a href=#pope>PoPE</a></li><li><a href=#longlora>LongLoRA</a></li></ol></li><li><a href=#evaluation-of-long-context-llms-长上下文大模型的评估标准>Evaluation of Long-Context LLMs 长上下文大模型的评估标准</a><ol><li><a href=#the-lost-in-the-middle-phenomenon-中间丢失现象>The Lost-in-the-Middle Phenomenon 中间丢失现象</a></li><li><a href=#long-context-benchmarks-长上下文的基准测试-niah-longbench>Long-Context Benchmarks 长上下文的基准测试: NIAH, LongBench</a></li></ol></li><li><a href=#efficient-attention-mechanismskv-cache-过大的问题>Efficient Attention Mechanisms，KV cache 过大的问题</a><ol><li><a href=#kv-cache>KV Cache</a></li><li><a href=#streamingllm-and-attention-sinks>StreamingLLM and Attention Sinks</a></li><li><a href=#duoattention-retrieval-heads-and-streaming-heads>DuoAttention: Retrieval Heads and Streaming Heads</a></li><li><a href=#quest-query-aware-sparsity>Quest: Query-Aware Sparsity</a></li></ol></li><li><a href=#beyond-transformers>Beyond Transformers</a><ol><li><a href=#state-space-models-ssms-mamba>State-Space Models (SSMs): Mamba</a></li><li><a href=#hybrid-models-jamba>Hybrid Models: Jamba</a></li></ol></li></ol></li><li><a href=#lec16-vit>Lec16 ViT</a><ol><li><a href=#basics-of-vision-transformer-vit>Basics of Vision Transformer (ViT)</a></li><li><a href=#efficient-vit--accerleration-techniques>Efficient ViT & accerleration techniques</a><ol><li><a href=#windows-attention>Windows attention</a></li><li><a href=#linear-attention>Linear attention</a></li><li><a href=#sparse-attention>Sparse attention</a></li></ol></li><li><a href=#self-supervised-learning-for-vit>Self-supervised learning for ViT</a><ol><li><a href=#contrastive-learning>Contrastive learning</a></li><li><a href=#masked-image-modeling>Masked image modeling</a></li></ol></li><li><a href=#vit--autoregressive-image-generation>ViT & Autoregressive Image Generation</a><ol><li><a href=#hybrid-autoregressive-transformer-hart>Hybrid Autoregressive Transformer (HART)</a></li></ol></li></ol></li><li><a href=#lec17-gan-video-point-cloud>Lec17 GAN, Video, Point Cloud</a><ol><li><a href=#efficient-gan>Efficient GAN</a><ol><li><a href=#gan-compression>GAN Compression</a></li><li><a href=#anycost-gan>AnyCost GAN</a></li><li><a href=#differentiable-augmentation-for-data-efficient-gans>Differentiable Augmentation for Data-Efficient GANs</a></li></ol></li><li><a href=#efficient-video-understanding>Efficient Video Understanding</a><ol><li><a href=#2d-cnn>2D CNN</a></li><li><a href=#3d-cnn>3D CNN</a></li><li><a href=#tsm-temporal-shift-module>TSM (Temporal Shift module)</a></li></ol></li><li><a href=#efficient-point-cloud-understanding>Efficient Point Cloud Understanding</a><ol><li><a href=#pvcnn--spvcnn>PVCNN / SPVCNN</a></li><li><a href=#bevfusion-birds-eye-view>BEVFusion (Bird&rsquo;s-Eye View)</a></li></ol></li></ol></li><li><a href=#lec18-diffusion-model>Lec18 Diffusion Model</a><ol><li><a href=#basics-of-diffusion-model>Basics of diffusion model</a><ol><li><a href=#denoising-diffusion-models>Denoising diffusion models</a></li><li><a href=#conditional-diffusion-models>Conditional diffusion models</a></li><li><a href=#latent-diffusion-models>Latent diffusion models</a></li><li><a href=#image-editing>Image editing</a></li><li><a href=#model-personalization>Model personalization</a></li></ol></li><li><a href=#fast-sampling-techniques>Fast sampling techniques</a><ol><li><a href=#denoising-diffusion-implicit-models>Denoising diffusion implicit models</a></li><li><a href=#distillation>Distillation</a></li></ol></li><li><a href=#acceleration-techniques>Acceleration techniques</a><ol><li><a href=#sparsity>Sparsity</a></li><li><a href=#quantization-1>Quantization</a></li><li><a href=#parallelism>Parallelism</a></li></ol></li></ol></li><li><a href=#lec19-distributed-training-1>Lec19 Distributed Training 1</a><ol><li><a href=#background-and-motivation>Background and motivation</a></li><li><a href=#parallelization-methods-for-distributed-trainging>Parallelization methods for distributed trainging</a></li><li><a href=#communication-primitives>Communication primitives</a></li><li><a href=#data-parallelism>Data Parallelism</a><ol><li><a href=#parameter-server>Parameter Server</a></li><li><a href=#去中心化的方法>去中心化的方法</a></li></ol></li><li><a href=#reducing-memory-in-data-parallelism-zero-123-and-fsdp>Reducing memory in data parallelism: Zero-1/2/3 and FSDP</a></li><li><a href=#pipeline-parallelism>Pipeline parallelism</a></li><li><a href=#tensor-parallelism>Tensor parallelism</a></li><li><a href=#sequence-parallelism>Sequence parallelism</a><ol><li><a href=#deepspeed-ulysses-solution-1-re-partition-data-in-attention-layers>DeepSpeed Ulysses (Solution 1: Re-partition data in Attention layers)</a></li><li><a href=#ring-attentionsolution-2-ring-attention>Ring Attention(Solution 2: Ring Attention)</a></li></ol></li></ol></li><li><a href=#lec20-distributed-training-2>Lec20 Distributed Training 2</a><ol><li><a href=#hybrid-mixed-parallelism-and-how-to-auto-parallelize>Hybrid (mixed) parallelism and how to auto-parallelize</a><ol><li><a href=#2d-parallelism>2D Parallelism</a></li><li><a href=#3d-parallelism>3D Parallelism</a></li><li><a href=#how-to-auto-parallelize>How to Auto Parallelize</a></li><li><a href=#alpa-a-unified-compiler-for-distributed-training>Alpa: A Unified Compiler for Distributed Training</a></li></ol></li><li><a href=#understand-the-bandwidth-and-latency-bottleneck-of-distributed-training>Understand the bandwidth and latency bottleneck of distributed training</a></li><li><a href=#gradient-compression-overcome-the-bandwidth-bottleneck>Gradient compression: overcome the bandwidth bottleneck</a><ol><li><a href=#gradient-prunning>Gradient Prunning</a></li><li><a href=#gradient-quantization>Gradient Quantization</a></li></ol></li><li><a href=#delayed-gradient-update-overcome-the-latency-bottleneck>Delayed gradient update: overcome the latency bottleneck</a><ol><li><a href=#bandwidth-vs-latency>Bandwidth vs. Latency</a></li></ol></li></ol></li><li><a href=#lec21-on-device-training-and-transfer-learning>Lec21 On-Device Training and Transfer Learning</a><ol><li><a href=#deep-leakage-fram-gradients-gradient-is-not-safe-to-share>Deep leakage fram gradients, gradient is not safe to share</a></li><li><a href=#memory-bottleneck-of-on-device-training>Memory bottleneck of on-device training</a></li><li><a href=#tiny-tansfer-learning-tinytl>Tiny tansfer learning (TinyTL)</a></li><li><a href=#sparse-back-propagation-sparsebp>Sparse back-propagation (SparseBP)</a></li><li><a href=#quantized-training-with-quantization-aware-scaling-qas>Quantized training with quantization aware scaling (QAS)</a></li><li><a href=#pockengine-system-support-for-sparse-back-propagation>PockEngine: system support for sparse back-propagation</a></li></ol></li><li><a href=#lec22-quantum-machine-learning-1>Lec22 Quantum Machine Learning 1</a><ol><li><a href=#single-qubit-state-and-gates>Single qubit state and gates</a><ol><li><a href=#single-qubit-state>Single qubit state</a></li><li><a href=#single-qubit-gates>Single Qubit Gates</a></li></ol></li><li><a href=#multiple-qubit-state-and-gates>Multiple-qubit state and gates</a><ol><li><a href=#multiple-qubit-state>Multiple-qubit state</a></li><li><a href=#multiple-qubit-gates>Multiple-qubit gates</a></li></ol></li><li><a href=#quantum-circuit>quantum circuit</a></li><li><a href=#the-nisq-era-and-compilation-problems>the NISQ era and compilation problems</a></li><li><a href=#the-example-workflow-and-compiler-on-neutral-atom-quantum-computer>the example workflow and compiler on neutral atom quantum computer</a></li></ol></li><li><a href=#lec23-quantum-machine-learning-2>Lec23 Quantum Machine Learning 2</a><ol><li><a href=#parameterized-quantum-circuit-pqc>Parameterized Quantum Circuit (PQC)</a></li><li><a href=#pqc-training>PQC Training</a></li><li><a href=#quantum-classifiers>Quantum Classifiers</a></li><li><a href=#noise-aware-on-chip-training-qoc-of-pqc>Noise Aware On-Chip Training (QOC) of PQC</a></li><li><a href=#torchquantum-library-for-qml>TorchQuantum Library for QML</a></li><li><a href=#robust-quantum-architecture-search>Robust Quantum Architecture Search</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/cover_hu_5a05ef640c1c6410.jpg srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/cover_hu_5a05ef640c1c6410.jpg 800w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/cover_hu_6516e40f043bd59b.jpg 1600w" width=800 height=450 loading=lazy alt="Featured image of post 『学习笔记』MIT6.5940-TinyML-and-Efficient-Deep-Learning-Computing"></a></div><div class=article-details><header class=article-category><a href=/categories/note/>笔记</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/>『学习笔记』MIT6.5940-TinyML-and-Efficient-Deep-Learning-Computing</a></h2><h3 class=article-subtitle>MIT6.5940 的学习笔记，QML 部分咕咕咕 TBD.</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>2025 年 6 月 14 日</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 31 分钟</time></div></footer></div></header><section class=article-content><blockquote><p>封面来源：<a class=link href=https://x.com/giname93076/ target=_blank rel=noopener>X(Twitter)@giname93076</a>
<sup><svg aria-hidden="true" focusable="false" viewBox="0 0 100 100" width="12" height="12" class="icon outbound"><path fill="currentColor" d="M18.8 85.1h56c2.2.0 4-1.8 4-4v-32h-8v28h-48v-48h28v-8h-32c-2.2.0-4 1.8-4 4v56C14.8 83.3 16.6 85.1 18.8 85.1z"/><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"/></svg></sup></p></blockquote><h2 id=lec01-引入>Lec01 引入</h2><h2 id=lec02-基础>Lec02 基础</h2><ul><li><p>Fully-Connected Layer (Linear Layer)</p><p>The output neuron is connected to all input neurons.</p></li><li><p>Convolution Layer</p><p>The output neuron is connected to input neurons in the receptive field.</p><p>1D conv, 2D conv, 还要在加上 channel 的维度</p><ol><li>feature map 特征图的大小变化（公式）</li><li>Padding 填充，zero padding, others (reflection, replication, constant &mldr;)</li><li>receptive field 感受野（公式）</li><li>strided，在不增加深度的情况下增大感受野</li><li>grouped conv, 减少计算量，初始版本，所有的 channel_i 和 channel_o 都是相连的，参数量会减少到原来的 g 倍（组数倍）</li><li>depthsise conv, 分组卷积的极限情况，</li><li>pooling layer，得到小的特征图，对高分辨率的图，max, average</li></ol></li><li><p>Normalization Layer</p><ol><li>BN, CNN, HW B</li><li>LN, atention, HW c</li><li>IN, HW</li><li>GN, HW g</li></ol></li><li><p>Activation Function</p><ol><li>sigmoid, 易于量化quantize，梯度消失</li><li>ReLU, 输入为正不会梯度消失，为负死了，易于实现稀疏性sparsify，不易量化</li><li>ReLU6，最大为6的ReLU，相对易于量化</li><li>Leaky ReLU，为负，失去稀疏性</li><li>Swish，x / (1 + e&-x)，硬件实现困难</li><li>Hard Swish<ul><li>0, &lt;= -3</li><li>x, >= 3</li><li>x * (x + 3) / 6</li></ul></li></ol></li></ul><h2 id=lab0>Lab0</h2><p>熟悉pytorch用法</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lr_lambda</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>step</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>interp</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=p>[</span><span class=n>step</span> <span class=o>/</span> <span class=n>steps_per_epoch</span><span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>num_epochs</span> <span class=o>*</span> <span class=mf>0.3</span><span class=p>,</span> <span class=n>num_epochs</span><span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>steps</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>steps_per_epoch</span> <span class=o>*</span> <span class=n>num_epochs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>steps</span><span class=p>,</span> <span class=p>[</span><span class=n>lr_lambda</span><span class=p>(</span><span class=n>step</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.4</span> <span class=k>for</span> <span class=n>step</span> <span class=ow>in</span> <span class=n>steps</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>LambdaLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>lr_lambda</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=lec03---04-剪枝>Lec03 - 04 剪枝</h2><ul><li><p>Pruning at different granularities</p><ol><li><p>fine-grained / unstructured, 细粒度剪枝，灵活，剪枝比率高，不好并行化</p></li><li><p>coarse-grained / structured,</p><p>1. pattern-based，提供几种模式，模式旋转等方式，规律性</p><p>N2M，N:M sparsity，不如 2:4，M个为一组，至少有N个被置零。</p><p>需要用两位来表示非零，为了稀疏，需要花费额外的内存来存储索引</p><p>2. vector-level 行</p><p>3. Kernel-level 一块</p><p>4. channel-level，拿掉一整个通道，加速简单，剪枝率低。</p><p>设计不同层的稀疏度，uniform shrink 均匀压缩；xxx</p><p>如何得到最佳稀疏度分配？AMC</p></li></ol></li><li><p>Pruning Criteria 剪枝标准</p><p>选最不重要的，heuristic 启发式</p><ol><li>magnitude-based pruning，基于权重大小，绝对值最小的</li><li>scaling-based pruning，给每一个滤波器一个缩放参数，或者是channel，学n个参数就行，然后再去除靠近零的filter，因为Batch Normalization 中有缩放因子scaling factor，可以用来复用</li><li>second-order-based pruning，泰勒展开 - 海森矩阵 - 近似</li><li>neurons to prune，实际是去掉一行，一块核</li><li>percentage-of-zero-based pruning，用ReLU的时候，会出现零，然后看激活值的零的占比，去掉占比最高的，需要运行，得到activation tensor</li><li>regression-based pruning，</li></ol></li><li><p>Finding pruning ratios</p><p>大部分都是假设层与层之间是独立的</p><ol><li>analyze the sensitivity of each layer，对每一层进行不同程度的剪枝，看准确率下降情况，设定降低5%~10%，画线对应过去，得到横坐标就是剪枝率</li><li>automatic pruning，自动剪枝<ol><li>AMC: AutoML for Model Compression，RL</li><li>NetAdapt, rule-based iterative/progressive method，设定减小的延迟latency，每一层看需要剪枝多少才能达成，后面进行short-term fine-tune，在能够得到一样的结果──减小设定的延迟的情况下，选择fine-tune后准确率最高的剪枝，不断迭代，最后整体进行 long-term fune-tune</li></ol></li></ol></li><li><p>Fine-tuning pruned neural networks</p><p>经验值，把学习率降低10~100倍</p><ol><li>iterative pruning，迭代剪枝，边剪枝边微调，为了70%，经过 30% - 50% - 70%</li></ol></li><li><p>System & Hardware Support for Sparsity</p><ol><li><p>EiE，权重稀疏 + 激活值稀疏？</p><p>对稀疏模型的硬件加速器设计</p></li><li><p>Tensor Core, M:N Weight Sparsity，相对规则，需要用2bit索引，乘法，用mask掩码</p></li><li><p>TorchSparsity & PointAcc，激活值稀疏，点云，稀疏卷积，不扩散，保持和输入的稀疏模式一致</p><p>自适应分组，MM & BMM</p><p>稀疏卷积硬件加速，归并排序找重叠部分</p></li></ol></li></ul><h2 id=lab1>Lab1</h2><p>实现 VGG 在 Cifar-10 模型的fine-grained pruning细颗粒剪枝与channel pruning通道剪枝。</p><p>同时应用了，sensitive敏感性排序，参数量排序等实际优化剪枝的方法</p><h2 id=lec05-量化>Lec05 量化</h2><ul><li><p>data type 数据类型，怎么样表示的</p><ol><li><p>IEEE FP32 1符号 + 8指数 + 23尾数，single precision</p></li><li><p>IEEE FP16 1符号 + 5指数 + 10尾数，half precision</p></li><li><p>Google BF16 ，1符号 + 8指数 + 7尾数，Brain Float，有时 FP32 -> FP16 训练不稳定，可以换成 BF 16</p></li><li><p>Nvidia FP8 (E4M3)，1符号 + 4指数 + 3尾数，hopper</p></li><li><p>Nvidia FP8 (E5M2)，1符号 + 5指数 + 2尾数</p><p>指数（数值范围、动态跨度大小），尾数（精度）</p></li><li><p>Nvidia INF4，1符号 + 3尾数，BlackWell</p><p>FP4 (E1M2), (E2M1), (E3M0)</p><p>E1M2 和 INT8一致，但是浪费应该 +- 0</p></li></ol></li><li><p>Quantization 量化</p><p>把输入从连续集合转换成离散数值集合的过程，之间的差异，称为量化误差，目标是最小化差异</p><ol><li><p>存储、计算：浮点数，浮点数</p></li><li><p>K-Means-based Quantization，code book</p><ul><li><p>概念</p><p>存储、计算：整数，浮点数</p><p>节省空间；计算量不变</p><p>存储的是代码本（k-means的质心）和分类的下标</p><p>N-bit quantization 量化，#parameters = M &#187; 2^N</p><p>32 bit x M = 32 M bit; N bit x M = NM bit + 32bit x 2^N = NM + 2^(N+5) bit</p><p>where 2^(N+5) bit can be ignored</p></li><li><p>量化后微调 fine-tune</p><p>得到梯度矩阵，把原本权重的分组，用在梯度矩阵上，求和，在权重的code book上去对应颜色的减掉 （乘学习率）</p><p>这个图有点神奇的，两个结合，但是得到了更好的结果：</p><img src=note.assets/image-20250406022031670.png alt=image-20250406022031670 style=zoom:50%><p>先 剪枝 后 量化，降低量化工作量，降低量化误差</p><p>低精度计算单元</p><p>经验值，Conv，在4bits后，才下降明显；FC，在2bits后才下降明显；所以4bits保持不错</p></li><li><p>其他的编码方式</p><p>Huffman Coding 哈夫曼编码</p><p>不同的权重出现的频率不同，变长编码策略</p><p>出现多的，用短编码</p><ul><li><p>three-stage pipeline Deep compression</p><p>深度压缩三阶段流水线</p><ol><li>剪枝，减少权重数量</li><li>量化，用k-means聚类算法，权重分组</li><li>编码，huffman coding，出现频率</li></ol></li></ul></li></ul><ol start=3><li><p>Linear Quantization</p><ul><li><p>概念</p><p>存储、计算：整数，整数</p><p>节省空间；减少计算量</p><p>原始参数权重 =></p><p>（量化后的参数权重 - zero point (int) ）* scale(float)</p><p>r(fp) = (q(int) - z(int)) * s(fp)</p><p>q_min max 是确定的，</p><p>s = (r_max - r_min) / (q_max - q_min)</p><p>z = round(q_min - r_min / S)</p></li><li><p>矩阵乘法运算</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406024808127.png width=1956 height=960 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406024808127_hu_3f799dbfe6d15a6.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406024808127_hu_92e6d94c59de2d5e.png 1024w" loading=lazy alt=image-20250406024808127 class=gallery-image data-flex-grow=203 data-flex-basis=489px></p><p>为了防止溢出，计算是需要类型转换</p><p>括号内后两项是常数（输入的零点，权重的量化值），包括括号外一项是常数，可以提前算</p><p>零点不变，量化权重不变</p><p>经验值，缩放因子在 (0, 1)，权重w的分布，遵循正态分布，Z_w = 0，为什么（？）</p><p>当 Z = 0，S = r_min / (q_min - Z) = - |r|_max / 2^(N-1)</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406025931073.png width=1956 height=959 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406025931073_hu_8321b7dd25241c56.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406025931073_hu_a57f1014fd1999db.png 1024w" loading=lazy alt=image-20250406025931073 class=gallery-image data-flex-grow=203 data-flex-basis=489px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406032859998.png width=2148 height=883 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406032859998_hu_c06488ecb2056d75.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406032859998_hu_43a5e04f2e165625.png 1024w" loading=lazy alt=image-20250406032859998 class=gallery-image data-flex-grow=243 data-flex-basis=583px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406032936357.png width=1767 height=1006 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406032936357_hu_a63f5c0dc3bcf1f1.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406032936357_hu_5146e2601fb6b8a8.png 1024w" loading=lazy alt=image-20250406032936357 class=gallery-image data-flex-grow=175 data-flex-basis=421px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406033029856.png width=1594 height=952 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406033029856_hu_dc435b33aedec7a9.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406033029856_hu_f977d1683c0a763c.png 1024w" loading=lazy alt=image-20250406033029856 class=gallery-image data-flex-grow=167 data-flex-basis=401px></p></li></ul></li></ol></li></ol></li></ul><h2 id=lec06-量化提高结果>Lec06 量化提高结果</h2><ul><li><p>Post-Training Quantization (PTQ)</p><p>quantization granularity</p></li><li><p>Quantization-Aware Training (QAT)</p></li><li><p>更低的量化位数</p><ol><li>binary quantization</li><li>ternary quantization</li></ol></li><li><p>automatic mixed-precision quantization 混合精度量化</p><p>每一层不一定要一样的精度</p></li></ul><h3 id=post-training-quantization>Post-training Quantization</h3><ul><li><p>Quantization Granularity</p><ol><li><p>Per-Tensor Quantization</p><p>对整个张量用一个缩放因子</p><p>大模型上效果好，小模型精度下降</p><p>原因：不同的channel的权重范围不一样</p></li><li><p>Per-Channel Quantization</p><p>更精细，误差更小，存储更多的值</p></li><li><p>Group Quantization，在4bit及以下，很重要</p><ul><li><p>VS-Quant: Per-Vector Quantization</p><p>全局浮点缩放因子，局部整数缩放因子</p><p>Multi-level scaling scheme 多级缩放</p></li><li><p>Shared Micro-exponent(MX) data type</p><p>L0 和 datatype 是共享的</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406103525669.png width=2169 height=429 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406103525669_hu_7c885041ecad1b1a.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406103525669_hu_19a72b08ee6b4e26.png 1024w" loading=lazy alt=image-20250406103525669 class=gallery-image data-flex-grow=505 data-flex-basis=1213px></p></li></ul></li></ol></li><li><p>Dynamic Range Clipping 动态范围裁剪</p><p>收集激活值的统计信息，在部署模型之前</p><ol><li><p>During Training 在训练的同时</p><p>Exponential Moving Averages (EMA)</p><p>维护 r_min, r_max，r(t) = alpha * r(t) + (1-alpha) * r(t-1)，平滑维护动态范围</p><p>（必须参与在训练）</p></li><li><p>calibration batch 训练后</p><p>不过可以使用多训练一个batch，用calibration校准数据集，估算动态范围</p></li></ol><p>可能不希望用真正的最大值</p><ol><li><p>最小化 MSE 均方误差</p><p>假设是高斯分布或者拉普拉斯分布，最两端的地方数量其实少，有对应封闭解</p><p>但实际符合这样分布的输入数据很少</p></li><li><p>最小化损失的信息</p><p>使用 KL divergence散度来校准量化范围</p></li></ol></li><li><p>Rounding 舍入</p><p>权重之间是相关的，舍入到最近的值不一定是最好的</p><ol><li><p>Round-to-Nearest</p></li><li><p>AdaRound</p><p>引入可学习的 delta 然后再四舍五入</p></li></ol></li></ul><h3 id=quantization-aware-training-qat>Quantization-Aware Training (QAT)</h3><p>量化感知训练，fine-tuning 恢复精度</p><p>K-means-based 量化，fine-tuning，更新质心即可</p><ul><li><p>线性量化？</p><p>Simulated quantization 模拟量化，fake quantization 伪量化</p><p>在训练的时候，维护一个全精度的参数权重，能累计非常小的梯度</p><p>再加上对激活值的量化的过程</p><p>增加这两个量化节点 Q(W), Q(Y)</p><p>训练好后，全精度参数权重就被抛弃</p><p>量化激活，阶跃的，梯度是0</p></li><li><p>Straight-Through Estimator (STE)</p><p>把weight-quantization node看成恒定函数 Y = X，传递梯度</p></li></ul><h3 id=binaryternary-quantization>Binary/Ternary Quantization</h3><ul><li><p>概念</p><p>Binary Weight Networks (BWN)</p><p>储存，计算：Binary/Ternary，Bit Operations</p><ol><li><p>deterministic binarization 确定性二值化</p></li><li><p>stochastic binarization 随机性二值化</p><p>需要随机数生成硬件</p></li></ol><p>精度下降大，量化误差大，再次引入缩放因子，1/n * |W|_1</p><p>啊？量化误差变化不大，精度能提升，从-21.2% 能到 0.2%？</p><p>激活值的二值化？</p><p>XNOR 同或，用他来代替乘法</p><p>默认值，0 是 -1，1 是 +2，起因也是有 XNOR 硬件计算快</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406133544987.png width=2412 height=1092 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406133544987_hu_a213636e07f04119.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406133544987_hu_8212bc1126c7b102.png 1024w" loading=lazy alt=image-20250406133544987 class=gallery-image data-flex-grow=220 data-flex-basis=530px></p><p>y = -n + popcount(W_i xnor x) &#171; 1</p></li><li><p>Ternary Weight Networks (TWN)</p><p>和 delta 比较，得到 +1 -1 0，经验值，0.7 * E(W)</p><p>同样缩放系数</p><p>Trained Ternary Quantization (TTQ)</p><p>可以再引入，正缩放系数 Wp 与负缩放系数 Wn</p></li><li><p>降低精度的时候，内存是线性下降，计算量，模型表达能力，二次下降</p></li></ul><h3 id=mixed-precision-quantization>Mixed-Precision Quantization</h3><p>混合精度量化</p><p>设计的空间很大</p><p>Hardward-aware automated quantization with mixed precision (HAQ)</p><h2 id=lab2>Lab2</h2><p>K-means Quantization</p><p>QAT，简化，k-means直接用权重再更新</p><p>训练/微调的时候是伪量化，部署时才是真量化</p><p>Linear Quantization</p><h2 id=lec07-nas-神经网络结构搜索>Lec07 NAS 神经网络结构搜索</h2><p>Neural Architecture Search (NAS)</p><p>不同于前面的训练、推理的优化，这是模型结构的优化</p><ol><li><p>ResNet，1x1 卷积，bottleneck block</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406221950762.png width=2010 height=858 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406221950762_hu_3847943bb3f661ac.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406221950762_hu_660c8b7a3a0bc52b.png 1024w" loading=lazy alt=image-20250406221950762 class=gallery-image data-flex-grow=234 data-flex-basis=562px></p></li><li><p>ResNeXt，1x1 分出来channel，分组卷积</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222006296.png width=2421 height=668 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222006296_hu_31198f564f4a78ac.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222006296_hu_36679652dab8a1d7.png 1024w" loading=lazy alt=image-20250406222006296 class=gallery-image data-flex-grow=362 data-flex-basis=869px></p></li><li><p>MobileNet: depthwise-separable block</p><p>空间信息depthwise和通道信息pointwise，分开区分</p><p>depthwise conv 表达能力弱</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222411507.png width=989 height=656 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222411507_hu_98c0b63e8df9b085.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222411507_hu_301b97eb076864e0.png 1024w" loading=lazy alt=image-20250406222411507 class=gallery-image data-flex-grow=150 data-flex-basis=361px></p></li><li><p>MobileNetV2: inverted bottleneck block</p><p>和bottleneck相反，中间增大</p><p>激活值的特性不好</p><p>可以用来减小模型大小和计算量，但激活内存不能（训练常常是激活内存为瓶颈）</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222421463.png width=937 height=587 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222421463_hu_bf8bfed1cdd6bd13.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250406222421463_hu_225b887efaaf1782.png 1024w" loading=lazy alt=image-20250406222421463 class=gallery-image data-flex-grow=159 data-flex-basis=383px></p></li><li><p>ShuffleNet</p><p>混洗shuffle，促进不同通道的信息的流动</p></li><li><p>Transformer</p><p>Multi-Head Self-Attention (MHSA)</p><p>感受野一层就可以全了</p></li></ol><ul><li><p>Search Space</p><ol><li><p>Cell-level search space</p><p>重复使用两种、</p><p>reduction cell 归约单元，降低分辨率</p><p>normal cell 普通单元</p></li><li><p>Network-level search space</p><ol><li>TinyML，内存更关键，在同样的内存限制下有更高FLOPs更好</li></ol></li></ol></li><li><p>搜索策略</p><ol><li><p>Grid search 网格搜索</p><p>需要训练，根据各个指标剔除</p><p>compound scaling 复合缩放</p></li><li><p>Random search</p><p>同样的搜索空间，但是随机变化，快速评估</p></li><li><p>Reinforcement learning</p><p>决策序列</p></li><li><p>Gradient descent</p><p>指标考虑，计算选择概率</p></li><li><p>Evoluitionary search 进化算法</p><p>变异、交叉等</p></li></ol></li><li><p>Performance Estimation Strategy</p><ol><li><p>Train from scratch</p><p>成本高</p></li><li><p>Inherit weight</p><p>从预训练的基础上，继承权重，拆分点，保持数学等价，改变深度、宽度</p><p>降低成本 net-to-net</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250407120550665.png width=2291 height=975 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250407120550665_hu_865a82921487f21c.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250407120550665_hu_188897bdda597596.png 1024w" loading=lazy alt=image-20250407120550665 class=gallery-image data-flex-grow=234 data-flex-basis=563px></p></li><li><p>Hypernetwork</p><p>用网络来预测网络参数，层作为node embedding</p><p>init embedding => final embedding 生成权重</p><p>用来降低训练成本</p></li></ol></li></ul><h2 id=lec08-nas-更高效>Lec08 NAS 更高效</h2><p>定制模型</p><p>前面的 NAS 太贵，选择proxy task代理任务，如更小的数据集，更少的训练轮数，FLOPs，参数量等</p><p>但是proxy task的相关性可能也没这么好。</p><ol><li><p>ProxylessNAS</p><p>路径级二值化，指走概率最高的路径</p><p>训练按概率，推理选概率最高</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412000504067.png width=2271 height=1085 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412000504067_hu_80806bdc0fad10a.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412000504067_hu_56a41b41cff6d39d.png 1024w" loading=lazy alt=image-20250412000504067 class=gallery-image data-flex-grow=209 data-flex-basis=502px></p><ul><li><p>MACs 不等于真实硬件效率</p><p>需要用真实硬件，效率低？并行！太贵？用延迟预测模型『架构，延迟』，最简单的模型是查询表，算子和延迟（一层一层的测延迟，相加）</p><p>GPU会有 kernel fusion，两个kernel 可能会变成一个kernel 而变快</p><p>计算密集型的两个，通畅不能kernel fusion，如两个矩阵乘法</p><p>但矩阵乘法 + 非线性激活函数是可以的，计算密集型 + 内存密集型</p><p>GPU会在更浅、更宽的表现好，CPU在更深、更细的表现好（对自己设备来说）</p></li><li><p>每个设备都要重新训练一个太贵，Once-For-All approach</p><p>同时训练多个模型？</p><p>用一个单一模型，包含许多子网络，稀疏激活</p><p>相比之前的重新训练，现在只需要在小型网络中抽取不同的subnetwork子网络就行了</p><p>设备不同，电量不同（适应不同能耗）等</p><ul><li><p>共享参数，不同子网络之间相互干扰？elastic 弹性的</p><ol><li>卷积核大小，不采用单独不同的卷积核大小，而是选择用变换矩阵处理，只用一个 7x7 的参数就好，小的参数都在7x7的内部</li><li>深度，shrink the depth 归约深度</li><li>通道，通过不同channel的magnitude幅值，对重要性进行排序，选择前 i 个通道</li></ol><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412003747820.png width=2265 height=1223 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412003747820_hu_d3d109dc2a9fa7da.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412003747820_hu_bfd68f8dba67072b.png 1024w" loading=lazy alt=image-20250412003747820 class=gallery-image data-flex-grow=185 data-flex-basis=444px></p></li><li><p>Roofline Analysis 屋顶线分析</p><p>折线图，X-获得一个字节的操作数，Y-GFLOPS 浮点算力</p><p>computation is cheap; memory is expensive.</p><p>内存瓶颈，计算瓶颈。</p></li></ul></li></ul></li><li><p>Zero-shot NAS</p><p>原本需要需要训练才知道评估acc准确率，变成只要看它的结构，推测是否能拿到高的准确率</p><p>ZenNAS, GradSign（感觉很直觉地开始套娃）</p><ol><li><p>ZenNAS 启发式</p><p>random weights 随机权重，粗略估计，结果不错</p><ol><li><p>随机初始化输入，符合正态分布</p></li><li><p>加入小的扰动</p></li><li><p>再把所有的权重，映射到正态分布</p></li><li><p>论文指出，z = log(f(x&rsquo;) - f(x))，如果模型效果好，应该对模型输入感到敏感，也就是说两个输出的差值应该大</p></li><li><p>+ batch normalization variance 批归一化（另一种启发式）</p><p>对于不同的批次，方差大好</p><p>计算每层的方差均值，加起来，希望这个方差越大越好</p><p>这样，不同的输出，容易得到不同的结果</p></li></ol></li><li><p>GradSign</p><p>好的模型会非常密集的sample-wise样本级局部最小值，两个局部最小值应该非常接近</p><p>在图中，绿色是梯度符号相同的部分，好的模型绿色部分应该更大，红色部分小</p><p>在初始点附近，随机选择些点，计数梯度符号相同的数量。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412010255963.png width=2558 height=1075 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412010255963_hu_a00edb8bda7c411f.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412010255963_hu_9ad5294ac72016f6.png 1024w" loading=lazy alt=image-20250412010255963 class=gallery-image data-flex-grow=237 data-flex-basis=571px></p></li></ol></li></ol><ul><li><p>Neural-hardware achitecture co-search，设计硬件</p><p>不仅搜索神经网络架构，也搜索加速器架构</p><p>硬件结构上会有些「非数值参数」需要设计，如连接性</p><ol><li><p>temporal mapping 时间映射</p><p>顺序处理</p></li><li><p>spatial parallelism 空间映射</p><p>空间并行处理</p></li></ol><p>两种 embedding，选择并行维度与顺序维度，分别按照重要性排序</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412031738639.png width=2032 height=1183 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412031738639_hu_61e7b9a6cdb26569.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412031738639_hu_1a1d55c3aba128ac.png 1024w" loading=lazy alt=image-20250412031738639 class=gallery-image data-flex-grow=171 data-flex-basis=412px></p></li><li><p>应用</p><ol><li><p>Once-for-ALL for Transformer and NLP</p><ul><li>HAT</li></ul></li><li><p>3D建模</p></li><li><p>GAN</p><p>小模型预览结果，大模型输出结果</p></li><li><p>Pose estimation</p></li><li><p>Quantum AI 量子</p><p>搜索最佳电路门</p></li></ol></li></ul><h2 id=lec09-知识蒸馏-kd>Lec09 知识蒸馏 KD</h2><p>Temperature 温度，高的温度，不同的区别越小，smooth，T在softmax的x => x / T</p><h3 id=匹配对齐什么>匹配/对齐什么？</h3><ol><li><p>对齐中间权重 matching intermediate weights</p><p>难点，维度不一样低秩近似/全连接/</p></li><li><p>中间特征 intermediate future / activation matching</p><p>激活值，中间的结果，也是相似的</p></li><li><p>梯度 Gradients</p><p>计算权重梯度或计算激活值梯度匹配</p><p>表现好的模型的注意力图是相似的</p></li><li><p>稀疏模式 sparsity patterns</p><p>来源于激活函数 ReLU 例如。</p></li><li><p>Relational information</p><ul><li>不同的层之间 C_in x C_out</li><li>不同样本之间，同一个模型，不同样本输入的不同输出之间的关系</li></ul></li></ol><h3 id=online-distillation-在线蒸馏>online distillation 在线蒸馏</h3><h4 id=self-distillation>self-distillation</h4><p>教师模型和学生模型架构一致</p><p>教师模型正常训练，学生模型用教师模型的交叉熵概率来训。</p><p>用前一步的作为教师模型，后一个以前一个为结果，</p><p>最后把所有模型ensemble，得一个更好的结果</p><h4 id=deep-mutual-learning-互学习-dml>Deep Mutual Learning 互学习 DML</h4><p>两个不一定相同的模型架构，互为师生，N1训练时，N2指导，反之亦然。</p><p>真实标签的交叉熵误差 + KL散度 两者结果</p><p>不需要预先训练，教师模型不一定要比学生模型大。</p><h5 id=combined-前面两种方法结合-be-your-own-teacher-deep-supervision--distillation>Combined 前面两种方法结合 Be Your Own Teacher: deep supervision + distillation</h5><p>用深层网络输出，作为浅层网络的教师，来自统一模型的不同部分。</p><p>蒸馏损失，在对真实标签的结果上，教师模型比学生模型的效果好时才能作为教师</p><p>物体识别，也可以看成（区域）分类</p><h5 id=增强小模型的效果>增强小模型的效果</h5><p>容易过拟合，做数据增强 cut out, mixup, dropout</p><p>容易欠拟合，做网络增强，NetAug，基础模型扩展</p><h2 id=lec10-mcunet-tinyml>Lec10 MCUnet TinyML</h2><h3 id=瓶颈>瓶颈</h3><p>参数数量，峰值激活，与，内存</p><h3 id=tinynas>TinyNAS</h3><p>Resolution 分辨率 和 Width Multipler 宽度调节因子</p><ol><li><p>Automated search space optimization 自动搜索空间优化</p><p>分析满足限制的模型的FLOPs分布，在各自的搜索空间中，高FLOPs=>高模型能力=>更可能高ACC</p><p>在同样的内存限制下，能有更高的运算量的设计空间更好</p><p>Flash 存储权重，SRAM 存储激活值</p><p>（最好的配比）</p><p>Flash↑，宽度调节因子（通道数）↑，分辨率↓，否则在 SRAM 中存不下 分辨率 x 通道数</p><p>SRAM↑，宽度调节因子基本不变，分辨率↑</p></li><li><p>Resource-constrained model specialization 资源有限的模型特化</p></li></ol><p>层的内存的峰值最小</p><h4 id=patch-based-inference-分块>Patch-based Inference 分块</h4><p>不再是 per-layer 整层输入输出，改为 per-patch，分成几部分输入输出</p><ul><li><p>坏处，增加了latency延迟，限制了并行能力（不过微控制器的并行能力是弱的）</p><p>卷积的重复计算，感受野，多了重叠的部分。感受野扩展</p></li><li><p>可以调整（减小早期的感受野，1x1，减少分块阶段的卷积层），总的需要不一样，在后面增加回卷积层，消除影响</p><p>早期用 分块推理，后期降下来，是正常推理</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412221709215.png width=1942 height=1064 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412221709215_hu_d5ceb53149cb9134.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250412221709215_hu_b31c4b62f807e858.png 1024w" loading=lazy alt=image-20250412221709215 class=gallery-image data-flex-grow=182 data-flex-basis=438px></p></li><li><p>再把这种分块推理的方式，放入搜索空间，推理调度</p></li><li><p>可以支持更大的输入分辨率</p></li></ul><h4 id=应用>应用</h4><ul><li><p>Tiny Vision</p><p>classification, visual wake words，检测任务 分辨率敏感（相比分类），所以分块推理，能使得分辨率提高</p><p>on-device training</p></li><li><p>Tiny Audio</p><p>二维语音，（时间，频率）功率，conv，相邻的频率、时间关联</p></li><li><p>Tiny time series/anomaly detection 微型时间序列异常检测</p><p>异常事件、产品（autoencoder，符合正常分布，重建误差小，不符合，误差大）</p><p>VLA，多个任务</p></li></ul><h2 id=lec11-tinyengine>Lec11 TinyEngine</h2><h3 id=loop-optimization-循环优化>Loop optimization 循环优化</h3><h4 id=loop-reordering-循环重排>Loop reordering 循环重排</h4><p>让访问内存更符合 cache line，连续访问</p><p>矩阵乘法 i, j, k => i, k, j，虽然输出访问变得不连续，但是还是会被cover掉</p><h4 id=loop-tiling-循环分块>Loop tiling 循环分块</h4><p>内存访问就 N*N => N*Tiling_size => Tiling_size * Tiling_size</p><p>内存局部性，降低缓存未命中</p><p>一般循环内层往外吧（？）</p><p>for ti, N block</p><p>​ for ti, ti + block</p><h5 id=两层缓存>两层缓存？</h5><p>设置第二层分块大小，多层次的分块 Tile2，和L2 cache 大小设计</p><h3 id=loop-unrolling-循环展开>Loop unrolling 循环展开</h3><p>分支预测，for 条件判定，循环展开，减少分支；但会增加重复代码，增加二进制文件大小</p><h3 id=simd-single-instruction-multiple-data-programming-单指令多数据>SIMD (single instruction, multiple data) programming 单指令多数据</h3><h4 id=isa-instruction-set-architecture-指令集架构>ISA (Instruction set architecture) 指令集架构</h4><h5 id=cisc-complex-instruction-set-computer-复杂指令集计算机>CISC (Complex Instruction Set Computer) 复杂指令集计算机</h5><p>Intel x86</p><p>并行处理范式</p><p>Vector Register，向量寄存器</p><p>Vector Operation，向量运算</p><p>提高吞吐量，速度</p><h5 id=risc-reduced-instruction-set-computer-精简指令计算机>RISC (Reduced Instruction Set Computer) 精简指令计算机</h5><p>Arm, RISC-V</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415131811589.png width=2487 height=1203 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415131811589_hu_bfac195046a81881.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415131811589_hu_27822bc7dcd050c4.png 1024w" loading=lazy alt=image-20250415131811589 class=gallery-image data-flex-grow=206 data-flex-basis=496px></p><h3 id=multithreading-多线程>Multithreading 多线程</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>pthread_t</span> <span class=n>threads</span><span class=p>[</span><span class=n>N</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>ThreadData</span> <span class=n>thread_data</span><span class=p>[</span><span class=n>N</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=n>pthread_create</span><span class=p>(</span><span class=o>&amp;</span><span class=n>threads</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=k>nullptr</span><span class=p>,</span> <span class=n>func</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>thread_data</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl><span class=n>pthread_join</span><span class=p>(</span><span class=n>threads</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=k>nullptr</span><span class=p>);</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=openmp>OpenMP</h4><p>编译器指令</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>omp_set_num_threads</span><span class=p>(</span><span class=mi>4</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=cp>#pragma omp parallel for
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span> <span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span> <span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span><span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>k</span> <span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=cuda>CUDA</h4><p>MMA 矩阵累加</p><h3 id=inference-optimization>Inference Optimization</h3><h4 id=image-to-column-im2col-convolution>Image to Column (Im2col) convolution</h4><h4 id=in-place-depth-wise-convolution>In-place depth-wise convolution</h4><h4 id=nhwc-for-point-wise-convolution-nchw-for-depth-wise-convolution>NHWC for point-wise convolution, NCHW for depth-wise convolution</h4><h4 id=winograd-convolution>Winograd convolution</h4><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415144734112.png width=1382 height=867 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415144734112_hu_fc3707c01f873052.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415144734112_hu_95f7a35d306d4505.png 1024w" loading=lazy alt=image-20250415144734112 class=gallery-image data-flex-grow=159 data-flex-basis=382px></p><h2 id=lec12-transfomer--llm>Lec12 Transfomer & LLM</h2><h3 id=transformer-基础>Transformer 基础</h3><p>&mldr;</p><h3 id=transfomer-design-variants-变体>Transfomer Design Variants 变体</h3><h4 id=encoder-decoder-t5>Encoder-Decoder (T5)</h4><h4 id=encoder-only-bert-bidirectional-encoder-representations-from-transformers>Encoder-only (BERT, Bidirectional Encoder Representations from Transformers)</h4><ul><li>Masked Language Model (MLM)</li><li>Next Sentence Prediction (NSP)</li></ul><h4 id=decoder-only-gpt-generative-pre-trained-transformer>Decoder-only (GPT, Generative Pre-trained Transformer)</h4><ul><li>Next word prediction</li></ul><h3 id=absoluterelative-positional-encoding>Absolute/Relative Positional Encoding</h3><ul><li><p>绝对位置编码</p><p>嵌入输入中</p><p>贯穿整个Transfomer过程</p></li><li><p>相对位置编码</p><p>只在注意力机制的部分</p><p>能处理更长的上下文，train short, test long</p><ol><li><p>ALiBi (Attention with Linear Biases)</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415165626626.png width=2434 height=1096 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415165626626_hu_a3a80bca31e310eb.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415165626626_hu_cce4a7c5cdd601d8.png 1024w" loading=lazy alt=image-20250415165626626 class=gallery-image data-flex-grow=222 data-flex-basis=532px></p></li><li><p>RoPE (Rotary Positional Embedding)</p><p>LLaMa</p><p>把长的嵌入转为二维的形式，(d1, d2)</p><p>interpolating 插值，当 m 翻倍，为了保持还能正常表示，theta / 2</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415165948390.png width=2482 height=1175 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415165948390_hu_b0c9e7b064708d70.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415165948390_hu_4cd97e10a119947b.png 1024w" loading=lazy alt=image-20250415165948390 class=gallery-image data-flex-grow=211 data-flex-basis=506px></p></li></ol></li></ul><h3 id=kv-cache-optimization>KV cache optimization</h3><p>需要 KV，才能在 Q 的时候，算出对应的 注意力</p><p>新token进来，没有 KV cache，则需要重算 KV？？？</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416021940775.png width=2060 height=836 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416021940775_hu_276de9e596bb8ffc.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416021940775_hu_b44a45eb58ddff04.png 1024w" loading=lazy alt=image-20250416021940775 class=gallery-image data-flex-grow=246 data-flex-basis=591px></p><h4 id=multi-head-attention-mha>Multi-Head Attention (MHA)</h4><p>n heads for query, n heads for key/value</p><p>KV cache 大小会乘以 n_kv，太大</p><h4 id=multi-query-attention-mqa>Multi-Query Attention (MQA)</h4><p>n heads for query, 1 head for key/value</p><p>会大大削弱模型能力</p><h4 id=grouped-query-attention-gqa>Grouped-Query Attention (GQA)</h4><p>折中</p><p>n heads for query, G heads for key/value (typically G = N/8)</p><p>在大模型下，准确率和 MHA 差不多</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415185619993.png width=1511 height=650 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415185619993_hu_cdbe65e25534f891.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415185619993_hu_7c7dd0e81fdd934.png 1024w" loading=lazy alt=image-20250415185619993 class=gallery-image data-flex-grow=232 data-flex-basis=557px></p><h3 id=ffn--swiglu-gated-linear-units>FFN => SwiGLU (Gated Linear Units)</h3><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415190529371.png width=2478 height=1256 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415190529371_hu_6d748348cb105390.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415190529371_hu_bc9c8a894461c46c.png 1024w" loading=lazy alt=image-20250415190529371 class=gallery-image data-flex-grow=197 data-flex-basis=473px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415190557019.png width=1671 height=636 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415190557019_hu_fbc13de481d682a9.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415190557019_hu_ed46eb22937941b9.png 1024w" loading=lazy alt=image-20250415190557019 class=gallery-image data-flex-grow=262 data-flex-basis=630px></p><h3 id=llm>LLM</h3><ul><li><p>LLaMa</p><ol><li><p>LLaMa</p><p>Decoder-only, Pre-norm,SwiGLU(swish,gatedlinearunits), rotary positional embedding (RoPE)</p><p>7B model_d 4096, 32 heads</p><p>65B model_d 8192, 64 heads</p></li><li><p>LLaMa 2</p><p>上下文更长 2k => 4k</p><p>GQA 分组询问注意力</p></li><li><p>LLaMa 3</p><p>多语言 token</p></li></ol></li><li><p>Mistral-7B</p><p>滑动窗口注意力机制，扩展上下文</p></li></ul><p>数据和模型参数一起变大。</p><h2 id=lec13-llm-deployment-techniques>Lec13 LLM Deployment Techniques</h2><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416154932613.png width=1500 height=405 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416154932613_hu_4629dca5a5e3d1f6.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416154932613_hu_a89f0ce5220f84ae.png 1024w" loading=lazy alt=image-20250416154932613 class=gallery-image data-flex-grow=370 data-flex-basis=888px></p><h3 id=quantization>Quantization</h3><h4 id=weight-activation-quantization-smoothquant>Weight-Activation Quantization: SmoothQuant</h4><p>前面提到的哪些朴素的量化方法对LLM，其实效果不好</p><p>原因：outliers 异常值，某些激活值很大，破坏精度</p><p>激活值，个别异常高的channel，蓝色部分将被舍入零；</p><p>权重值，一般都比较小，ez。</p><p>取舍，smooth bond：</p><p>考虑到权重和激活值是线性矩阵运算，所以，比如激活值乘 0.1，权重乘 10，结果不变。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415203628745.png width=2357 height=716 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415203628745_hu_c8019d96b6258a98.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415203628745_hu_7301060d77b88839.png 1024w" loading=lazy alt=image-20250415203628745 class=gallery-image data-flex-grow=329 data-flex-basis=790px></p><ol><li><p>Calibration Stage</p><p>找到激活值 col_max，找到权重 row_max，相除得到缩放因子 s = \sqrt(col_max / row_max)</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415204653698.png width=1846 height=740 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415204653698_hu_8ed85037706828ec.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415204653698_hu_27d9d8bdc794959c.png 1024w" loading=lazy alt=image-20250415204653698 class=gallery-image data-flex-grow=249 data-flex-basis=598px></p></li><li><p>Smoothing Stage</p><p>应用缩放因子</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415204804801.png width=2009 height=997 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415204804801_hu_cb8f097f8318385e.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415204804801_hu_2d84a295e22df8c5.png 1024w" loading=lazy alt=image-20250415204804801 class=gallery-image data-flex-grow=201 data-flex-basis=483px></p></li><li><p>Inference (deployed model) 部署</p><p>没有再缩放，编译的时候处理了（fuse 到前一层）</p></li></ol><p>为什么单节点比分布式好，communication overhead</p><h4 id=weight-only-quantization-awq-and-tinychat>Weight-Only Quantization: AWQ and TinyChat</h4><p><strong>W4A16</strong> for Single-batch single user server</p><p>单用户，就是 batchsize 是 1，计算瓶颈是 weight</p><p>weight在边缘设备的LLM推理中的影响</p><ol><li>上下文与生成阶段，生成阶段是瓶颈</li><li>生成阶段受限于内存通讯</li><li>weight的占用内存的大小，比activation大多了</li></ol><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415213903708.png width=2480 height=912 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415213903708_hu_4317370f47500f48.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415213903708_hu_c7e14618b1682f9d.png 1024w" loading=lazy alt=image-20250415213903708 class=gallery-image data-flex-grow=271 data-flex-basis=652px></p><h5 id=awq-activation-aware-weight-quantization>AWQ: Activation-aware Weight Quantization</h5><p>传统 RTN（Round To Nearest）FP16 => INT3，clip()，降低很多。</p><p>？？？</p><p>只要保留一行，即1%channel，的关键权重，幻觉显著下降！</p><p>怎么找出这 1% 呢？</p><p>在量化权重的过程中，不关注权重的情况，而是关注激活值的情况！</p><p>因为下一层的激活值，是由权重与上一层的激活值相乘得出，所以，激活值大的，保留，</p><p>也是前面说的少量的异常值outlier</p><p>但是同个张量中出现fp16 和 int8，很难实现，会引入<strong>混合精度</strong>的计算，变得麻烦。</p><p>其实是不必要引入的，借用前面SmoothQuant中用到的方法，把权重的敏感性转给我们保持不变的激活值</p><p>相当于增加一位的精度</p><p>不需要反向传播，不需要基于回归的方法，只需要 calibration 校准数据集。</p><p>（Perplexity 困惑度 是衡量语言模型质量的一个指标，和真是输出的比较，越小越好）</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415215452151.png width=987 height=735 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415215452151_hu_abd5625cd096b908.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250415215452151_hu_9f49148ab4a7987d.png 1024w" loading=lazy alt=image-20250415215452151 class=gallery-image data-flex-grow=134 data-flex-basis=322px></p><h5 id=tinychat-llm-inference-engine-on-edge>TinyChat: LLM Inference Engine on Edge</h5><h6 id=hardware-aware-packing>Hardware-aware packing</h6><p>怎么解决 4bit 和 1字节 对不齐的问题？</p><p>改变存储方式，为了更好地解码，交错存储</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416005829003.png width=2120 height=1223 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416005829003_hu_e53f3da8838e0869.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416005829003_hu_9d95431e6f77e382.png 1024w" loading=lazy alt=image-20250416005829003 class=gallery-image data-flex-grow=173 data-flex-basis=416px></p><h6 id=kernel-fusion>Kernel Fusion</h6><p>Kernel call 很贵，做融合，BMM，批量矩阵乘法</p><h4 id=qserve-w4a8kv4>QServe (W4A8KV4)</h4><h5 id=背景-融合两者的优点>背景-融合两者的优点</h5><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416011051694.png width=2262 height=1186 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416011051694_hu_1e4762963fa833ea.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416011051694_hu_11f4bc1cc52011e5.png 1024w" loading=lazy alt=image-20250416011051694 class=gallery-image data-flex-grow=190 data-flex-basis=457px></p><h5 id=smoothattention>SmoothAttention</h5><p>类似与SmoothQuant，Q 是平滑的，K 会有某些通道有outlier异常值</p><h5 id=反量化由于溢出可能要调整计算方式>反量化，由于溢出可能要调整计算方式</h5><p>改变位数之后，负数的话，乘一个数，可能下溢出了，所以可以先乘再加减</p><p>先缩放还是先加减。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416015019363.png width=2553 height=1068 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416015019363_hu_bd68e0615a9d4888.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416015019363_hu_4bc965debe1f9f59.png 1024w" loading=lazy alt=image-20250416015019363 class=gallery-image data-flex-grow=239 data-flex-basis=573px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416015228315.png width=2280 height=841 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416015228315_hu_bb97ce608897171f.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416015228315_hu_f49349b7f1960417.png 1024w" loading=lazy alt=image-20250416015228315 class=gallery-image data-flex-grow=271 data-flex-basis=650px></p><h3 id=pruning--sparsity>Pruning & Sparsity</h3><h4 id=weight-sparsity-wanda>Weight Sparsity: Wanda</h4><p>传统：看权重本身magnitude</p><p>Wanda：关注最终激活值小的，对应的权重</p><h4 id=contextual-sparsity>Contextual Sparsity</h4><h5 id=dejavu-input-dependednt-sparsity>DejaVu (input dependednt sparsity)</h5><p>？</p><h5 id=moe-mixture-of-experts>MoE (Mixture-of-Experts)</h5><p>提高总参数，不提高推理代价</p><p>router路由器分配workload工作</p><h6 id=路由机制>路由机制</h6><p>token选择expert</p><p>expert选择token</p><p>全局expert分配</p><h4 id=attention-sparsity>Attention Sparsity</h4><h5 id=spatten-token-pruning--head-pruning>SpAtten (token pruning & head pruning)</h5><p>Q-K，K列的attention sum，大 = 重要</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416020822222.png width=1109 height=746 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416020822222_hu_c41a80c45078338.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416020822222_hu_ea14401ca7bf8051.png 1024w" loading=lazy alt=image-20250416020822222 class=gallery-image data-flex-grow=148 data-flex-basis=356px></p><h5 id=h2o-token-pruning-in-kv-cache>H2O: token pruning in KV cache</h5><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416021040545.png width=1175 height=461 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416021040545_hu_b7e60311140dc4b4.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416021040545_hu_ea227e730cbd177c.png 1024w" loading=lazy alt=image-20250416021040545 class=gallery-image data-flex-grow=254 data-flex-basis=611px></p><h3 id=llm-serving-systems>LLM Serving Systems</h3><h4 id=important-metrics-指标-for-llm-serving>Important Metrics 指标 for LLM Serving</h4><ul><li>Time To First Token (TTFT)，响应速度，实时互动</li><li>Time Per Output Token (TPOT)，每个token所需时间 100 ms/token, 10 token/s</li><li>Latency = (TTFT) + (TPOT * number of token to be generated)，总延迟</li><li>Throughput，对所有请求的每秒产生的 token 数</li></ul><h4 id=优化目标>优化目标</h4><p>最小 TTFT，最大 throughput，减小 TPOT，后两个需要 tradeoff，常矛盾</p><p>常用启发式：输出长度，输入长度，模型大小</p><h4 id=paged-attention-vllm>Paged Attention (vLLM)</h4><h5 id=kv-cache--的资源浪费>KV Cache 的资源浪费</h5><ol><li>Internal fragmentation：内部碎片化，由于不知道输出长度，过度分配空间</li><li>Reservation：预留碎片化，现在步骤没用，未来会用</li><li>External fragmentation：多个request，不知道sequence长度，要空出位置</li></ol><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416022438962.png width=2560 height=544 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416022438962_hu_dc7a22d451f3a388.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416022438962_hu_30335e7ea2f2f239.png 1024w" loading=lazy alt=image-20250416022438962 class=gallery-image data-flex-grow=470 data-flex-basis=1129px></p><h5 id=解决--pagedattention的好处>解决 / PagedAttention的好处</h5><p>由 OS 操作系统的 virtual memory and paging 虚拟内存和分页机制启发</p><p>交替使用 KV blocks</p><ol><li>解决 <strong>KV-cache</strong> 内存碎片化，支持<strong>多访问</strong> requests</li><li>动态块映射 使得 能够 <strong>共享 Prompt</strong></li></ol><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416022728222.png width=2453 height=1014 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416022728222_hu_a1e4d8e3ac93974c.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416022728222_hu_ad9e4bcc996de33d.png 1024w" loading=lazy alt=image-20250416022728222 class=gallery-image data-flex-grow=241 data-flex-basis=580px></p><h4 id=flashattention>FlashAttention</h4><p>生成attention注意力矩阵时，NxN 很大</p><p>tiling + kernel fusion</p><h4 id=speculative-decoding-推测性解码>Speculative Decoding 推测性解码</h4><p>小模型 Draft model，生成</p><p>大模型 Target model，验证</p><p>小模型自回归生成，大模型并行验证（因为大模型运行比较贵）</p><p>纠正，重新生成</p><h4 id=batching>Batching</h4><p>增加吞吐量</p><ul><li>no batching，不做批处理</li><li>static batching，静态批处理，固定批次大小</li><li>dynamic batching，动态批处理，批次大小到了，或者时间到了</li><li>continuous batch (in-flight batch)，连续批处理，token级别</li></ul><h2 id=lec-14-llm-post-training>Lec 14 LLM Post-Training</h2><h3 id=llm-fine-tuning-微调>LLM Fine-Tuning 微调</h3><h4 id=supervised-fine-tuning-sft-监督微调>Supervised Fine-Tuning (SFT) 监督微调</h4><p>对齐人类价值观/偏好，比如说话更加友好，更加善解人意</p><p>helpfulness & safety</p><h4 id=reinforcement-learning-from-human-feedback-rlhf-基于人类反馈的强化学习>Reinforcement Learning from Human Feedback (RLHF) 基于人类反馈的强化学习</h4><p>BLEU、ROUGE的测试，客观答案，RLHF 更加主观，人类定义的创造性、可信的、有用的</p><h5 id=朴素的>朴素的</h5><p>奖励模型训练──数据生成结果，人类对不同结果排序，比较函数，排序前的大大大于后的</p><p>两方面</p><ol><li>调整后的模型，不会过拟合奖励模型，和原始模型的内容不能偏差过多</li><li>奖励模型下的结果不错，符合人类偏好</li></ol><p>三个模型，两个损失值</p><h5 id=direct-preference-optimization-dpo-直接偏好优化>Direct Preference Optimization (DPO) 直接偏好优化</h5><p>简化流程，转化为单流程的 SFT 任务</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416123850866.png width=1516 height=290 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416123850866_hu_297eae2994f1aebf.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250416123850866_hu_c911b00705af73b3.png 1024w" loading=lazy alt=image-20250416123850866 class=gallery-image data-flex-grow=522 data-flex-basis=1254px></p><h4 id=parameter-efficient-fine-tuning-peft>Parameter Efficient Fine-Tuning (PEFT)</h4><h5 id=bitfit-fine-tune-only-the-bias-terms-只微调偏置项>BitFit (Fine-tune only the bias terms) 只微调偏置项</h5><p>微调权重需要存储激活值，但微调偏置项不需要存储激活值</p><h5 id=tinytl-lite-residual-learning>TinyTL: Lite Residual Learning</h5><p>在主干网络计算量较大的基础上，添加轻量级的侧分支，只更新侧分支，学习残差</p><p>下采样 group conv, 1x1 conv，上采样，激活规模小</p><h5 id=adapter-插入适配器层>Adapter 插入适配器层</h5><p>Adapter Layer：残差，下采样 激活 上采样，bottleneck</p><p>对每个任务，只添加一些可训练的参数</p><p>会增加模型深度，增加计算开销，延迟增加</p><p>不改变模型？</p><h5 id=prompt-tuning>Prompt Tuning</h5><p>可以训练连续的prompt，学习prompt</p><h5 id=prefix-tuning>Prefix-Tuning</h5><p>Prompt-Tuning 只对第一层有提示 => 对每一层有提示</p><p>增加输入损失，KV cache 使用变大，延迟变大</p><p>不引入额外推理延迟？</p><h5 id=lora>LoRA</h5><p>同样训练侧分支</p><p>从 d 维 => 低秩 r 维（高斯分布初始化），低秩 r 维 => d 维（零初始化）</p><p>最初添加，不会有影响</p><p>h = x @ W + x @ A @ B = x @ (W + A @ B) = x @ W'</p><p>没有非线性激活，所以可以fuse到原本的矩阵乘法</p><h5 id=qlora>QLoRA</h5><p>同样 LoRA 的设计原则，加上对骨架模型的量化</p><ol><li>引入 NormalFloat (NF4)，centroid 不是学到的，是固定的</li><li>双重量化 Double quantization，缩放因子也被量化</li></ol><p>CPU卸载功能的分页优化器，优化状态不用时，存放在CPU，节省内存</p><h5 id=bit-delta>Bit-Delta</h5><p>Your Fine-Tune May Only Be Worth One Bit</p><p>出发点是，模型已经学得很好了，微调只需要加一点点参数就好</p><p>能不能就微调 1 位，把增量量化至一位，还有一个缩放因子</p><p>二值化delta，sin(delta) > 0 => 1 else -1</p><h3 id=multi-model-llms>Multi-model LLMs</h3><h4 id=cross-attention-based-flamingo>Cross-Attention Based: Flamingo</h4><p>将视觉信息注入inject到语言模型</p><p>LLM 参数固定，加入cross-attention layers</p><p>视觉信息 KV，文本信息 Q</p><h4 id=visual-tokens-as-input-palm-e-vila>Visual Tokens as Input: PaLM-E, VILA</h4><p>全部都 tokenize，视觉信息tokens</p><p>解冻LLM参数；</p><p>交错使用图文，而不是图文对，否则LLM性能下降严重；</p><p>混合数据，还是需要纯文本数据</p><p>分辨率重要</p><p>高分辨率的处理，分块多少，看任务，OCR 分块多好；知识推理不一定</p><p>QKV，把低分辨率作为 Q，高分辨率作为 KV</p><h4 id=enabling-visual-outputs-vila-u>Enabling Visual Outputs: VILA-U</h4><p>统一图像和文字理解</p><h3 id=prompt-engineering>Prompt Engineering</h3><h4 id=in-context-learning-icl>In-Context Learning (ICL)</h4><p>zero-shot few-shot</p><h4 id=chain-of-thought-cot>Chain-of-Thought (CoT)</h4><p>let&rsquo;s think step by step</p><h4 id=retrieval-augmented-generation-rag>ReTrieval Augmented Generation (RAG)</h4><h2 id=lec15-long-context-llm>Lec15 Long-Context LLM</h2><h3 id=context-extension>Context Extension</h3><h4 id=pope>PoPE</h4><p>增加频率，扩展上下文，然后还需要去微调 Fine-tune</p><h4 id=longlora>LongLoRA</h4><p>性能瓶颈：注意力机制。二次增长</p><p>偏移稀疏注意力，不同模式，作为一个注意力头</p><p>怎么Fine-Tune embedding 和 normalization 层的？</p><p>两个模式都用，比单用一个模式好。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250425163300680.png width=2463 height=1101 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250425163300680_hu_6079ce95bb3f2080.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250425163300680_hu_cf1ca00561cfb455.png 1024w" loading=lazy alt=image-20250425163300680 class=gallery-image data-flex-grow=223 data-flex-basis=536px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250425164007742.png width=1927 height=246 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250425164007742_hu_b6a2065cc5c2cde3.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250425164007742_hu_6520be9708dbb469.png 1024w" loading=lazy alt=image-20250425164007742 class=gallery-image data-flex-grow=783 data-flex-basis=1880px></p><h3 id=evaluation-of-long-context-llms-长上下文大模型的评估标准>Evaluation of Long-Context LLMs 长上下文大模型的评估标准</h3><h4 id=the-lost-in-the-middle-phenomenon-中间丢失现象>The Lost-in-the-Middle Phenomenon 中间丢失现象</h4><p>当相关信息在开头和结尾时，准确率高，中间准确率低。</p><p>**生成一段流畅的长上下文回复，不意味着模型真正记住了里面的内容，**所以只用困惑度是不够的。</p><h4 id=long-context-benchmarks-长上下文的基准测试-niah-longbench>Long-Context Benchmarks 长上下文的基准测试: NIAH, LongBench</h4><h5 id=niah-needle-in-a-haystack-大海捞针>NIAH (Needle In A Haystack) 大海捞针</h5><p>aa在bb干了cc。做询问</p><p>随着上下文的变长，询问在文章xx%的位置的内容needle，检索Retrival准确率。</p><p><strong>人为设计的合成基准测试</strong></p><h5 id=longbench>LongBench</h5><p>多种任务，发现上下文压缩等技术不如位置编码。</p><p><strong>现实世界的测试</strong></p><h3 id=efficient-attention-mechanismskv-cache-过大的问题>Efficient Attention Mechanisms，KV cache 过大的问题</h3><h4 id=kv-cache>KV Cache</h4><p>BS * layers * kv-heads * n_emd * length * 2 * type，每个token</p><h4 id=streamingllm-and-attention-sinks>StreamingLLM and Attention Sinks</h4><p>保持恒定内存，Window Attention 的问题，第一个token被移出时，PPL上升</p><p>Dense Attention 的问题，在token长度超过预训练长度时，PPL上升 perplex</p><p>滑动窗口 + Re-computation 重计算</p><h6 id=attention-sink-注意力汇聚-现象>Attention Sink 注意力汇聚 现象</h6><p>对<strong>第一个token</strong>的注意力会高。</p><p>用了softmax，注意力得分和为1，就算有些不需要关注，而自回归模型中，首个token是全局可见的，所以把这些冗余的注意力得分给它。</p><p>是因为semantic <strong>语义</strong>，还是position <strong>位置</strong>？是位置。</p><p><strong>保留来一个可训练的注意力汇聚点 / 四个注意力汇聚点。</strong></p><p>（实验得出四个是 sweet point）</p><p>ViT 的注意力汇聚点出现在语义信息比较少的区域。</p><p>Bert 在句子末尾的分隔符标记</p><p><strong>streamingLLM不等同于长上下文，查询早期的是查不到的，在kv cache中淘汰了。</strong></p><p>(DuoAttention 是来解决这个问题)</p><h4 id=duoattention-retrieval-heads-and-streaming-heads>DuoAttention: Retrieval Heads and Streaming Heads</h4><p>Duo = Two，<strong>同样不能无限长，但是能够减缓</strong>。</p><p>retrieval head 和 streaming head</p><p>retrieval head，最初的 dense attention</p><p>streaming head，只关注 recent token & reduced tokens</p><p>每个注意力头都需要训alpha</p><p>因为是要用更少的内存，所以，我们对<strong>这个注意力结果做蒸馏distill</strong>，使得和最终的差值最小。</p><p><strong>需要训练多少个alpha？</strong></p><p>layers x heads</p><p><strong>训练材料？</strong></p><p>类似于NIAH，设置一系列 passkey。</p><p><strong>推理的时候怎么办？</strong></p><p>设置阈值threshold，大于dense，小于streaming。</p><p><strong>decoding</strong></p><p>两种 kv cache 一个是全部，一个是sink point + 最近几个token</p><p>计算是正常的多头。</p><p><strong>prefilling</strong></p><p>分块注意力</p><p>time complexity $O(L^2) \to O(LK)$</p><p>memory complexity $O(L) \to O(K)$</p><p>希望 streaming head 越多，节省的越多。</p><p>实验中，有一半可以作为streaming head。</p><p>实际上是<strong>对attention的剪枝、稀疏化</strong>。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250430150408915.png width=2045 height=499 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250430150408915_hu_6fc78555e7c8a5f0.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250430150408915_hu_e53338f72f47b63.png 1024w" loading=lazy alt=image-20250430150408915 class=gallery-image data-flex-grow=409 data-flex-basis=983px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250430150615629.png width=2282 height=654 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250430150615629_hu_bff49cc4150d77b0.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250430150615629_hu_f989fa1b169dc28b.png 1024w" loading=lazy alt=image-20250430150615629 class=gallery-image data-flex-grow=348 data-flex-basis=837px></p><h4 id=quest-query-aware-sparsity>Quest: Query-Aware Sparsity</h4><ol><li>Dense Attention</li><li>Query-Agnostic Sparsity 查询无关，要是在前一个token除移除了kv，后面的不会再有这个toekn</li><li>Query-Aware Sparsity，查询感知，前一个移除了，不影响后面还是可以有；基于正在解码的新词元。</li></ol><p>因为确实会有某个token对前一个来说不重要，但对下一个很重要的情况，所以我们要全都存下来kv cache，<strong>因此没有节省内存，只是节省移动的内存开销</strong>，只抓取重要的 kv cache，其他的留在内存中。</p><p>同样的对 attention page 求和/求平均，只抓取重要的page，其余的留在内存</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250501025912215.png width=1721 height=758 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250501025912215_hu_57f92e3e7fa6bb40.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250501025912215_hu_d4eb82400d9a689c.png 1024w" loading=lazy alt=image-20250501025912215 class=gallery-image data-flex-grow=227 data-flex-basis=544px></p><h3 id=beyond-transformers>Beyond Transformers</h3><h4 id=state-space-models-ssms-mamba>State-Space Models (SSMs): Mamba</h4><p>注意力机制两大任务，不同之间，单个内部。 <strong>还是不懂#</strong></p><p>Mamba，加基础上引入 Selective State Spafe)</p><p>固定的kv cache，不线性增长。</p><h4 id=hybrid-models-jamba>Hybrid Models: Jamba</h4><p>混合模型。</p><h2 id=lec16-vit>Lec16 ViT</h2><h3 id=basics-of-vision-transformer-vit>Basics of Vision Transformer (ViT)</h3><p>Patch（CNN, patch_size, 3, hidden_dim），Position Encoding，然后就和语言模型一样了</p><p>对比CNN，数据量小的时候，CNN好，大的时候，ViT好。</p><h3 id=efficient-vit--accerleration-techniques>Efficient ViT & accerleration techniques</h3><p>超分辨率，有实时应用场景；</p><p>高分辨率，对自动驾驶重要。</p><p>高分辨率，对比CNN，ViT 的计算量提升很快，是二次方的提升，分辨率也是二次方，所以就是四次方。</p><p>Segment Anything</p><h4 id=windows-attention>Windows attention</h4><p>注意力机制只在窗口window内发生，固定token大小，计算复杂度的是线性的。</p><p>但这样一来，注意力就在局部流通，全局没有了？</p><p><strong>Swin Transformer</strong> 引入 <strong>shift window</strong>，shift operation，让下一层的窗口移动，使得能注意到相邻窗口的内容。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527163636223.png width=902 height=521 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527163636223_hu_9bb43053f771ff19.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527163636223_hu_65cf25c84bdeef2.png 1024w" loading=lazy alt=image-20250527163636223 class=gallery-image data-flex-grow=173 data-flex-basis=415px></p><h5 id=sparse-windows-attention>Sparse Windows attention</h5><p>并不是所有的windows都是有用的。</p><p><strong>FlatFormer</strong>，相比于 等窗口组合，用 等大小组合 ，可以更加硬件友好，更好地并行，不多等待。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527163650238.png width=1899 height=645 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527163650238_hu_19d57f9b8c96a3ca.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527163650238_hu_83dd10e65d49ddf6.png 1024w" loading=lazy alt=image-20250527163650238 class=gallery-image data-flex-grow=294 data-flex-basis=706px></p><h4 id=linear-attention>Linear attention</h4><p>替换。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527164046256.png width=2434 height=1103 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527164046256_hu_b0d3d8b3dc1accff.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527164046256_hu_5f009dcd26ad77b.png 1024w" loading=lazy alt=image-20250527164046256 class=gallery-image data-flex-grow=220 data-flex-basis=529px></p><p>然后发现效果差很多，注意力不突出了。</p><p>擅长捕捉全局上下文信息，但局部信息不行。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527165047146.png width=2503 height=1088 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527165047146_hu_6e43812a2f1135c.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527165047146_hu_220d57c1a5531eb7.png 1024w" loading=lazy alt=image-20250527165047146 class=gallery-image data-flex-grow=230 data-flex-basis=552px></p><p>想到CNN是提取局部信息的好工具，在原先的基础上，加上CNN</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527205318607.png width=2424 height=688 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527205318607_hu_219d85b78f22a9e3.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527205318607_hu_c3c32659b636d10.png 1024w" loading=lazy alt=image-20250527205318607 class=gallery-image data-flex-grow=352 data-flex-basis=845px></p><p>结果提升。（分析新的注意力分布与原本的注意力分布特征的区别，得出的解决方案）</p><h4 id=sparse-attention>Sparse attention</h4><p><strong>SparseViT</strong>，用 L2 激活值来确定窗口的重要程度。</p><p>分出不同的重要程度，可以在不同层使用不同的稀疏度。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527205729628.png width=2495 height=969 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527205729628_hu_46d1510dec38fac5.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527205729628_hu_f0fada0766d3f7c8.png 1024w" loading=lazy alt=image-20250527205729628 class=gallery-image data-flex-grow=257 data-flex-basis=617px></p><h3 id=self-supervised-learning-for-vit>Self-supervised learning for ViT</h3><p>怎么利用unlabeled data</p><h4 id=contrastive-learning>Contrastive learning</h4><p>拿同一张图片的不同 crop，去做同一的、拉近的 loss，不同的图片做拉远的 loss。</p><p>在小数据集上训的时候，SL，更大的模型可能得不到更好的结果，但是用了对比学习自监督self-SL（CL）会更好</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527213845273.png width=2154 height=1308 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527213845273_hu_3ecd920e2eecce9a.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527213845273_hu_3431e3c37c1c0094.png 1024w" loading=lazy alt=image-20250527213845273 class=gallery-image data-flex-grow=164 data-flex-basis=395px></p><p>多模态对比学习 <strong>CLIP</strong></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527214423449.png width=1936 height=1175 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527214423449_hu_6a8f7867d22e3f40.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527214423449_hu_5f38de318095fbfd.png 1024w" loading=lazy alt=image-20250527214423449 class=gallery-image data-flex-grow=164 data-flex-basis=395px></p><h4 id=masked-image-modeling>Masked image modeling</h4><p>类似与bert的重建遮挡，Mask Language Models, MLM。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527215151473.png width=2446 height=1185 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527215151473_hu_c9f6bc9a3f567fdc.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527215151473_hu_85855e8fd97ee5f0.png 1024w" loading=lazy alt=image-20250527215151473 class=gallery-image data-flex-grow=206 data-flex-basis=495px></p><p>Heavy encoder只编码未遮的图片，lite会编码所有。</p><p>mask 70~75% sweet spot</p><p>作为对比，bert 比率是 15%，图片冗余大。</p><h3 id=vit--autoregressive-image-generation>ViT & Autoregressive Image Generation</h3><p>Autoregressive AR。</p><h4 id=hybrid-autoregressive-transformer-hart>Hybrid Autoregressive Transformer (HART)</h4><p>和新目标是减少迭代次数来加速。</p><p>有三种不同的生成方式。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527220043178.png width=2560 height=1147 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527220043178_hu_7be52bf7a8cf119a.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527220043178_hu_b80baa76c9a6dd8b.png 1024w" loading=lazy alt=image-20250527220043178 class=gallery-image data-flex-grow=223 data-flex-basis=535px></p><p>文字生成和图像生成的不同，语言有词汇表，离散的，而图像是连续的。</p><p>要用一种AR架构把这两种模态统一起来，就需要一种离散的图像标记，就可以使用同样的loss了。</p><p>具体的，加入vector quantized, VQ encoder/decoder和 codebook，一个像素的vector量化是一个标量（量化）。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527220442830.png width=2249 height=1158 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527220442830_hu_13e1d2a242348d6e.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527220442830_hu_1df5e57a10919bf4.png 1024w" loading=lazy alt=image-20250527220442830 class=gallery-image data-flex-grow=194 data-flex-basis=466px></p><p>经验法则：一次性生成更多的标记token。</p><p>Visual Autoregressive，<strong>VAR</strong>，引入新的标记生成方法。</p><p>先为一张图像生成一个token，和分成2x2&mldr;，多个粒度。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221348853.png width=2345 height=1136 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221348853_hu_581321086b9e42b9.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221348853_hu_6ae4d039a8969ad3.png 1024w" loading=lazy alt=image-20250527221348853 class=gallery-image data-flex-grow=206 data-flex-basis=495px></p><p>他的 attention mask，也有变化（没特别理解</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221517972.png width=2461 height=1014 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221517972_hu_261d90ba519cb7e1.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221517972_hu_4200f1478c90e94.png 1024w" loading=lazy alt=image-20250527221517972 class=gallery-image data-flex-grow=242 data-flex-basis=582px></p><p>不过效果没那么好。</p><p>Hybrid Image Tokenization，<strong>HART</strong></p><p>小的duffusion model，<strong>residual duffusion</strong>残差扩散，来学习离散token和连续token的区别（因为离散token自己学细粒度的很困难）</p><p>训练时候采样50% 50%，让两者在 decoder 中处于同一空间</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221835339.png width=2528 height=1193 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221835339_hu_dfbb8c936074bfcf.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527221835339_hu_980cd3d3f6daeb0c.png 1024w" loading=lazy alt=image-20250527221835339 class=gallery-image data-flex-grow=211 data-flex-basis=508px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527222340548.png width=2560 height=801 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527222340548_hu_a07e2ce693b561b0.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527222340548_hu_9db7c94c679df433.png 1024w" loading=lazy alt=image-20250527222340548 class=gallery-image data-flex-grow=319 data-flex-basis=767px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527222609940.png width=2425 height=894 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527222609940_hu_fd3c3664d2e5644.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250527222609940_hu_8b67a659dbdc0890.png 1024w" loading=lazy alt=image-20250527222609940 class=gallery-image data-flex-grow=271 data-flex-basis=651px></p><h2 id=lec17-gan-video-point-cloud>Lec17 GAN, Video, Point Cloud</h2><h3 id=efficient-gan>Efficient GAN</h3><p>显然为了加速推理，压缩 generator</p><p>un/conditional GAN</p><p>提供条件（class, segmentation map, strokes/随机噪声</p><p>GAN 比识别的模型贵</p><h4 id=gan-compression>GAN Compression</h4><p>重建reconstruction loss，蒸馏（中间特征图）distillation loss，cGAN loss（真实图片和生成图片）</p><h4 id=anycost-gan>AnyCost GAN</h4><p>StyleGAN2只采样最高分辨率，MSG-GAN采样所有分辨率，随机采样</p><p>不同通道数量，增加蒸馏损失，可以使得删去通道后的图片样式类似</p><p>同样的判别器对于不同的分辨率效果不一定都好</p><h4 id=differentiable-augmentation-for-data-efficient-gans>Differentiable Augmentation for Data-Efficient GANs</h4><p>需要收集很多数据，贵</p><p>图片增强</p><p>只对真实图片增强，颜色改变，图片位置shift，部分cutout，会导致生成的图片也长这样，所以不好</p><p>（训练D的时候）在生成后都应用，判别器对转换后的图片的判别率高，对原图片 G，效果不好</p><p>在训练（G和D的时候）都判别前运用图片转换</p><h3 id=efficient-video-understanding>Efficient Video Understanding</h3><p>temporal modeling 时间建模</p><h4 id=2d-cnn>2D CNN</h4><p>采样图片，再aggregate，average max</p><p>双流网络 spatial + temporal，optical flow</p><p>2D CNN + Post-fusion(e.g. LSTM) ，low level 是独立处理的</p><p>好处，计算高效，重复利用图片识别2D CNN</p><p>坏处，时间信息，光流计算量大，late fusion 无法建模 low level</p><h4 id=3d-cnn>3D CNN</h4><p>C3D，参数量变大</p><p>I3D，用2D CNN来初始化3D CNN，inflation，就重复</p><p>好处，时空信息一起 ，各个级别的信息都可以建模</p><p>坏处，模型大小，计算量都变大</p><h4 id=tsm-temporal-shift-module>TSM (Temporal Shift module)</h4><p>不用计算量、参数来为时间建模</p><p>offline，bi-direction 可以做双向</p><p>online，uni-direction 做单向</p><p>shift 的比例，不能太多也不能太少</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250601153930427.png width=1436 height=536 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250601153930427_hu_6ad4098158a39e88.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250601153930427_hu_ea789585a809da1b.png 1024w" loading=lazy alt=image-20250601153930427 class=gallery-image data-flex-grow=267 data-flex-basis=642px></p><h3 id=efficient-point-cloud-understanding>Efficient Point Cloud Understanding</h3><p>稀疏，非规整；应用场景算力限制</p><h4 id=pvcnn--spvcnn>PVCNN / SPVCNN</h4><p>Point-Voxel，Point local，Voxel global（稀疏掉0，让 point 去做高粒度）</p><p>3D NAS SPV</p><h4 id=bevfusion-birds-eye-view>BEVFusion (Bird&rsquo;s-Eye View)</h4><p>Dense 摄像头，Sparse 雷达，产生BEV + 3D 对象检查</p><h2 id=lec18-diffusion-model>Lec18 Diffusion Model</h2><h3 id=basics-of-diffusion-model>Basics of diffusion model</h3><h4 id=denoising-diffusion-models>Denoising diffusion models</h4><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144431428.png width=2460 height=1317 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144431428_hu_4c9849eb4d03cc76.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144431428_hu_fe1f8a0e686762e5.png 1024w" loading=lazy alt=image-20250612144431428 class=gallery-image data-flex-grow=186 data-flex-basis=448px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144612691.png width=2143 height=530 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144612691_hu_af8dd42a3d6e0904.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144612691_hu_496728e371b5621b.png 1024w" loading=lazy alt=image-20250612144612691 class=gallery-image data-flex-grow=404 data-flex-basis=970px></p><p>训练算法</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144858452.png width=2363 height=1065 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144858452_hu_386987a032bd871c.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612144858452_hu_ad5c432142e70e58.png 1024w" loading=lazy alt=image-20250612144858452 class=gallery-image data-flex-grow=221 data-flex-basis=532px></p><p>采样算法</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612145637150.png width=2535 height=1206 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612145637150_hu_4b99adf85c10d09.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612145637150_hu_3a37a941badb5d28.png 1024w" loading=lazy alt=image-20250612145637150 class=gallery-image data-flex-grow=210 data-flex-basis=504px></p><h4 id=conditional-diffusion-models>Conditional diffusion models</h4><h5 id=scalar-condition>Scalar condition</h5><p>Class ID，encode，embedding，加到特征图上（或者embedding scale 和 bias更加复杂）
<img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150234787.png width=2339 height=1143 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150234787_hu_71da306afb27c118.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150234787_hu_966cfad1a55d2501.png 1024w" loading=lazy alt=image-20250612150234787 class=gallery-image data-flex-grow=204 data-flex-basis=491px></p><h5 id=text-condition>Text condition</h5><h6 id=cross-attention>Cross Attention</h6><p>图像和文本并不对称，图片 Q，文本 K V</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150219729.png width=2315 height=988 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150219729_hu_667fab28f0805788.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150219729_hu_fd9b09f37ef00e9e.png 1024w" loading=lazy alt=image-20250612150219729 class=gallery-image data-flex-grow=234 data-flex-basis=562px></p><h6 id=joint-attention>Joint Attention</h6><p>文本和图像对称</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150515135.png width=2467 height=1023 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150515135_hu_110f64eff5f7e32f.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150515135_hu_704b293914362bd9.png 1024w" loading=lazy alt=image-20250612150515135 class=gallery-image data-flex-grow=241 data-flex-basis=578px></p><h6 id=single-self-attention>Single Self Attention</h6><p>Early fusion</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150613830.png width=1778 height=922 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150613830_hu_9486d50f30767f82.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150613830_hu_9cb1c012468a4a20.png 1024w" loading=lazy alt=image-20250612150613830 class=gallery-image data-flex-grow=192 data-flex-basis=462px></p><h5 id=pixel-wise-condition>Pixel-wise condition</h5><p>Control Net</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150841930.png width=2480 height=928 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150841930_hu_b3d5a4ab739d27e5.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612150841930_hu_a281c2b82c87a8e6.png 1024w" loading=lazy alt=image-20250612150841930 class=gallery-image data-flex-grow=267 data-flex-basis=641px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612151008236.png width=2309 height=1188 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612151008236_hu_3b2bd718a74684a6.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612151008236_hu_41410bd422f6b51e.png 1024w" loading=lazy alt=image-20250612151008236 class=gallery-image data-flex-grow=194 data-flex-basis=466px></p><p>关于多样性和质量，增加 c 的分类器，强度</p><p>classifier-free guidance</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612151941365.png width=2476 height=1188 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612151941365_hu_d407e2b5001a1c17.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612151941365_hu_336ef96be56acc34.png 1024w" loading=lazy alt=image-20250612151941365 class=gallery-image data-flex-grow=208 data-flex-basis=500px></p><h4 id=latent-diffusion-models>Latent diffusion models</h4><p>较少计算量</p><p>预训练 VAE，编码到潜空间 diffusion，最后再解码</p><p>学习目标是一样的，预测噪音</p><p>采样也是类似</p><p>分辨率压缩的越多，运行的越快</p><h5 id=deep-compression-autoencoder-dc-ae-f64>Deep Compression Autoencoder (DC-AE) f64</h5><p>压缩 64 倍，考虑 Attention 平方，减少的计算有 4k 倍</p><p>具体地，通过显式 space-to-channel / channel-to-space，残差自编码，使得更加稳定</p><p>因为自编码器要的计算量变大，为了减少计算量，使用分层稀疏调优的方式，减少计算量</p><p>还有 Linear Attention，使用小 LLM 作为文本编码器，kernel fusion，flow based PPM 求解器</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612154044926.png width=2110 height=1057 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612154044926_hu_7c42e0313c571d0e.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612154044926_hu_461e724981ea8eb1.png 1024w" loading=lazy alt=image-20250612154044926 class=gallery-image data-flex-grow=199 data-flex-basis=479px></p><h4 id=image-editing>Image editing</h4><h5 id=stroke-base-editing>Stroke-Base Editing</h5><p>通过增加噪声，使得草图和图像接近，然后解出来（具体训练是怎么样的？）</p><p>SDEdit</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612155420892.png width=2375 height=811 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612155420892_hu_8e4cc2a362f5e20c.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612155420892_hu_d064d7abb01559cd.png 1024w" loading=lazy alt=image-20250612155420892 class=gallery-image data-flex-grow=292 data-flex-basis=702px></p><h4 id=model-personalization>Model personalization</h4><p>人物一致性</p><p>DreamBooth，通过 finetune，用特别的标识符来代表这个类别</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612155712527.png width=2381 height=1152 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612155712527_hu_67bded98bfde30bd.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612155712527_hu_14afe4c4e8004013.png 1024w" loading=lazy alt=image-20250612155712527 class=gallery-image data-flex-grow=206 data-flex-basis=496px></p><p>但是，只能对每一个新的类别都需要去finetune，costly</p><p>后续也有 training-free 的技术</p><h3 id=fast-sampling-techniques>Fast sampling techniques</h3><p>能否增大步幅，减少步骤</p><h4 id=denoising-diffusion-implicit-models>Denoising diffusion implicit models</h4><p>之前的马尔可夫Markovian 只依赖前一个步骤，增加和 x0 的关系</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160144368.png width=2213 height=1117 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160144368_hu_eefef8c252c397ba.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160144368_hu_105f32547e4feaa7.png 1024w" loading=lazy alt=image-20250612160144368 class=gallery-image data-flex-grow=198 data-flex-basis=475px></p><h4 id=distillation>Distillation</h4><p>渐进蒸馏，教师模型一步一步，学生模型从教师模型的两步里面蒸馏学习成一步，然后渐进蒸馏，就可以减少步数</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160241743.png width=2130 height=1325 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160241743_hu_e135cefd6a1da6a6.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160241743_hu_c36cdf870d34dbb1.png 1024w" loading=lazy alt=image-20250612160241743 class=gallery-image data-flex-grow=160 data-flex-basis=385px></p><h3 id=acceleration-techniques>Acceleration techniques</h3><h4 id=sparsity>Sparsity</h4><p>编辑只编辑了一些，但需要对所有像素进行运算</p><p>SDEdit，只重新计算改变的部分，别的部分复用</p><p>Sparse Incremental Generative Engine (SIGE)</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160639455.png width=2441 height=1247 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160639455_hu_d6a9aebfe5f15ed3.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612160639455_hu_5d5fee72aeea92fb.png 1024w" loading=lazy alt=image-20250612160639455 class=gallery-image data-flex-grow=195 data-flex-basis=469px></p><p>Image Inpainting，类似，同时可以实现交互式</p><h4 id=quantization-1>Quantization</h4><p>SVDQuant</p><p>和 LLM 不同，Diffusion Model 是compute-bound，所以 weight-only quantization 没办法加速扩散模型</p><p>使用类似 SmoothQuant 的方式，把激活值的 outlier 转移到权重上，然后权重使用 side (low rank) branch 去全精度保持精度损失，经过 SVD，异常值减少W4A4</p><p>同时，如果使用 LoRA funetuning，就不需要重新量化，在原本的全精度上追加秩就行了</p><p>简单实现，会带来不小的其他开销，kernel fusion 把旁支的 kernel 合在原本的 kernel 中，由于他们共享输入/输出</p><h4 id=parallelism>Parallelism</h4><p>DistriFusion，相邻时间戳的输入实际上很相似，可以通过通信旧的激活值，来 overlap 通信与计算</p><p>同时，在更高的分辨率下，加速比更高，因为通信开销更大。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612161913159.png width=2363 height=1041 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612161913159_hu_ed18db285ee356b.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250612161913159_hu_ac76f82f3f8298a.png 1024w" loading=lazy alt=image-20250612161913159 class=gallery-image data-flex-grow=226 data-flex-basis=544px></p><h2 id=lec19-distributed-training-1>Lec19 Distributed Training 1</h2><h3 id=background-and-motivation>Background and motivation</h3><p>模型大，对于单 GPU 来说训练时间太长，需要多 GPU 协同训练</p><h3 id=parallelization-methods-for-distributed-trainging>Parallelization methods for distributed trainging</h3><ul><li><p>Data parallelism</p><p>拆分数据，多个 GPU 上的模型权重是共享的</p><p>partition data, sharing model</p></li><li><p>Pipeline Parallelism</p><p>拆分模型，一份数据。</p><p>按 layer-dimension 划分</p></li><li><p>Tensor Parallelism</p><p>拆分模型，一份数据。</p><p>按 激活值 来划分</p></li><li><p>Sequence Parallelism</p><p>data parallelism 是 batch，sequence parallelism 是 token</p></li></ul><h3 id=communication-primitives>Communication primitives</h3><ul><li><p><strong>One-to-One: Send and Recv</strong></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613165612939.png width=1962 height=1081 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613165612939_hu_d4fcbd4f319089e7.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613165612939_hu_133f2af03898a92d.png 1024w" loading=lazy alt=image-20250613165612939 class=gallery-image data-flex-grow=181 data-flex-basis=435px></p></li><li><p><strong>One-to-Many and Many-to-One: Scatter and Gather</strong></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613165825185.png width=2322 height=1055 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613165825185_hu_a5d6eda6815286c6.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613165825185_hu_f0fac2f144570faa.png 1024w" loading=lazy alt=image-20250613165825185 class=gallery-image data-flex-grow=220 data-flex-basis=528px></p></li><li><p><strong>Many-to-One and One-to-Many: Reduce and Broadcast</strong></p><p>Reduce 可以看作是 Gather + Reduce 归约操作</p><p>Broadcast 是把张量的全部都分发给所有节点，Scatter 是把不同部分分发给不同节点</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613172900306.png width=2323 height=1156 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613172900306_hu_b9aaa7151ea0703a.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613172900306_hu_7b5f438c01c4650c.png 1024w" loading=lazy alt=image-20250613172900306 class=gallery-image data-flex-grow=200 data-flex-basis=482px></p></li><li><p><strong>Many-to-Many: All-Reduce and All-Gather</strong></p><p>All-Reduce 对所有 workers 做 Reduce</p><p>All-Gather 对所有 workers 做 Gather</p></li></ul><h3 id=data-parallelism>Data Parallelism</h3><h4 id=parameter-server>Parameter Server</h4><p>中心化</p><ol><li>Workers pull model from Server</li><li>Workers push & sum to Server gradient</li><li>Server update model using gradient</li><li>Workers replicate / pull the updated model to update local copy</li></ol><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613164631712.png width=2508 height=1297 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613164631712_hu_44a83d3756b6b902.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613164631712_hu_e1bda9e5ec8296cc.png 1024w" loading=lazy alt=image-20250613164631712 class=gallery-image data-flex-grow=193 data-flex-basis=464px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200509743.png width=2323 height=975 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200509743_hu_f270c48467681b30.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200509743_hu_d759adaabd3ad813.png 1024w" loading=lazy alt=image-20250613200509743 class=gallery-image data-flex-grow=238 data-flex-basis=571px></p><h4 id=去中心化的方法>去中心化的方法</h4><ul><li><p>Naive All-Reduce, Sequential</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200734771.png width=2194 height=1125 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200734771_hu_e4150071f0ac17f1.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200734771_hu_ed4c3329af763a79.png 1024w" loading=lazy alt=image-20250613200734771 class=gallery-image data-flex-grow=195 data-flex-basis=468px></p></li><li><p>Better All-Reduce, Ring</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200800292.png width=2473 height=1113 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200800292_hu_a57ce8d6af8016.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613200800292_hu_3e3c58361e0f1fcd.png 1024w" loading=lazy alt=image-20250613200800292 class=gallery-image data-flex-grow=222 data-flex-basis=533px></p></li><li><p>Naive All-Reduce, Parallel Reduce</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201020226.png width=2413 height=1114 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201020226_hu_55e97c8644f0f44d.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201020226_hu_d84e6a8de0c13d0f.png 1024w" loading=lazy alt=image-20250613201020226 class=gallery-image data-flex-grow=216 data-flex-basis=519px></p></li><li><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201121123.png width=1884 height=634 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201121123_hu_92926dfef16c07c1.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201121123_hu_16e804aed20a0525.png 1024w" loading=lazy alt=image-20250613201121123 class=gallery-image data-flex-grow=297 data-flex-basis=713px></p></li><li><p>Recursive Halving All Reduce (Butterfly All Reduce)</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201351600.png width=2458 height=1135 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201351600_hu_5135e62de8c3a435.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613201351600_hu_25fadad1749f3052.png 1024w" loading=lazy alt=image-20250613201351600 class=gallery-image data-flex-grow=216 data-flex-basis=519px></p></li></ul><h3 id=reducing-memory-in-data-parallelism-zero-123-and-fsdp>Reducing memory in data parallelism: Zero-1/2/3 and FSDP</h3><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613202114941.png width=2448 height=1267 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613202114941_hu_5f384b2a345bcd48.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613202114941_hu_919c0cd187b17a6e.png 1024w" loading=lazy alt=image-20250613202114941 class=gallery-image data-flex-grow=193 data-flex-basis=463px></p><h3 id=pipeline-parallelism>Pipeline parallelism</h3><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613212924989.png width=2404 height=1172 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613212924989_hu_8e53d8ff76c7119e.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613212924989_hu_c4c6c32adc158858.png 1024w" loading=lazy alt=image-20250613212924989 class=gallery-image data-flex-grow=205 data-flex-basis=492px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613213026648.png width=2409 height=1158 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613213026648_hu_8f9e5f90e1a80b69.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613213026648_hu_a4cd5a6ad7dded2b.png 1024w" loading=lazy alt=image-20250613213026648 class=gallery-image data-flex-grow=208 data-flex-basis=499px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613213054283.png width=2379 height=994 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613213054283_hu_84a1d2cdabd6bd19.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613213054283_hu_dacd269c8198030f.png 1024w" loading=lazy alt=image-20250613213054283 class=gallery-image data-flex-grow=239 data-flex-basis=574px></p><h3 id=tensor-parallelism>Tensor parallelism</h3><p>从 d_dim 维度切，垂直切，再水平切</p><p>Scatter and All-Reduce</p><p>broadcast activation</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613214711506.png width=2400 height=1306 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613214711506_hu_a07381a63091d056.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613214711506_hu_2230fb72f7835f79.png 1024w" loading=lazy alt=image-20250613214711506 class=gallery-image data-flex-grow=183 data-flex-basis=441px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613214001072.png width=2094 height=1064 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613214001072_hu_240946dd6073a058.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613214001072_hu_446ff4c39cc4f32c.png 1024w" loading=lazy alt=image-20250613214001072 class=gallery-image data-flex-grow=196 data-flex-basis=472px></p><h3 id=sequence-parallelism>Sequence parallelism</h3><p>处理长下文</p><p>比如把一本书的不同章节分别做，但是注意力不能互相计算，只是局部的话，会缺失上下文。</p><h4 id=deepspeed-ulysses-solution-1-re-partition-data-in-attention-layers>DeepSpeed Ulysses (Solution 1: Re-partition data in Attention layers)</h4><p>All-to-All 全对全通信开销大，节点之间通信成本高；</p><p>最大并行度？会受到模型的多头注意力的头的个数</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613215341718.png width=2478 height=1347 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613215341718_hu_5139d54be6ec48cc.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613215341718_hu_b1fde7d1ed26c759.png 1024w" loading=lazy alt=image-20250613215341718 class=gallery-image data-flex-grow=183 data-flex-basis=441px></p><h4 id=ring-attentionsolution-2-ring-attention>Ring Attention(Solution 2: Ring Attention)</h4><p>交换 KV1 KV2 KV3</p><p>并行度不再受 head_num 限制</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613220222536.png width=2248 height=1042 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613220222536_hu_6a4567cf0091b1c7.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250613220222536_hu_ca81ca5fb67b545d.png 1024w" loading=lazy alt=image-20250613220222536 class=gallery-image data-flex-grow=215 data-flex-basis=517px></p><p>Longvilla，结合这两种方法，在一个节点中，用 Ulysses，节点之间用 Ring Attention。</p><p>（节点内部通信高）</p><h2 id=lec20-distributed-training-2>Lec20 Distributed Training 2</h2><h3 id=hybrid-mixed-parallelism-and-how-to-auto-parallelize>Hybrid (mixed) parallelism and how to auto-parallelize</h3><h4 id=2d-parallelism>2D Parallelism</h4><ul><li><p>Outer: DP</p><p>Inner: PP</p></li><li><p>Outer: PP</p><p>Inner: TP</p></li><li><p>Intra-node: all-to-all repartition</p><p>Inter-node: ring attention</p></li></ul><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614012437333.png width=1910 height=1107 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614012437333_hu_6f693826f0f91bda.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614012437333_hu_10197c568b844492.png 1024w" loading=lazy alt=image-20250614012437333 class=gallery-image data-flex-grow=172 data-flex-basis=414px></p><h4 id=3d-parallelism>3D Parallelism</h4><ul><li>PP + TP + DP</li></ul><h4 id=how-to-auto-parallelize>How to Auto Parallelize</h4><p>模型太大，不能放单机；PP</p><p>模型层太大，不能方单机；TP</p><h4 id=alpa-a-unified-compiler-for-distributed-training>Alpa: A Unified Compiler for Distributed Training</h4><p>搜索空间大，分层搜索空间 Hierarchical Space。</p><p>Inter-op Parallelism</p><p>Intra-op Parallelism</p><p>Cost，计算成本、通信成本、数据重分布成本</p><p>（那还有说法吗？这个设计）</p><h3 id=understand-the-bandwidth-and-latency-bottleneck-of-distributed-training>Understand the bandwidth and latency bottleneck of distributed training</h3><p>通信很重要。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614013903028.png width=2121 height=467 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614013903028_hu_ca07c8b785348800.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614013903028_hu_40298beacfbd3352.png 1024w" loading=lazy alt=image-20250614013903028 class=gallery-image data-flex-grow=454 data-flex-basis=1090px></p><p><strong>估算延迟</strong></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614014017059.png width=2213 height=1198 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614014017059_hu_f58ca66a5b265b56.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614014017059_hu_4313c5c87ac60be7.png 1024w" loading=lazy alt=image-20250614014017059 class=gallery-image data-flex-grow=184 data-flex-basis=443px></p><h3 id=gradient-compression-overcome-the-bandwidth-bottleneck>Gradient compression: overcome the bandwidth bottleneck</h3><h4 id=gradient-prunning>Gradient Prunning</h4><h5 id=sparse-communication-稀疏通信>Sparse Communication 稀疏通信</h5><p>结合局部梯度累积的梯度剪枝</p><ul><li><p>只 send top-k 梯度 by magnitude</p></li><li><p>保持未 send（没有到 top-k 的）作为 error feedback (residual)</p><p>保留残差，直到累积到阈值 （梯度裁切）</p></li></ul><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614021915785.png width=1153 height=542 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614021915785_hu_df335455ec4a0161.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614021915785_hu_59ba5690634140fb.png 1024w" loading=lazy alt=image-20250614021915785 class=gallery-image data-flex-grow=212 data-flex-basis=510px></p><p>导致性能下降。</p><ul><li><p>Momentum 动量机制</p><p>直接累积梯度，会导致优化方向的偏移</p><p><strong>应该累积速度，而非梯度</strong></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614014930010.png width=1773 height=869 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614014930010_hu_c0dd3cf88bcd543d.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614014930010_hu_a1213cc14908ac35.png 1024w" loading=lazy alt=image-20250614014930010 class=gallery-image data-flex-grow=204 data-flex-basis=489px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614015158377.png width=1695 height=788 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614015158377_hu_500013976071b3f5.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614015158377_hu_61c41ce8123f8908.png 1024w" loading=lazy alt=image-20250614015158377 class=gallery-image data-flex-grow=215 data-flex-basis=516px></p></li></ul><h5 id=deep-gradient-compression>Deep Gradient Compression</h5><p>warm up training</p><p>在训练早期，权重改变大；warm up learning rate</p><p>累积梯度会加剧问题；warm up sparsity</p><p>指数逐渐增大，保持稳定。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614015736570.png width=725 height=529 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614015736570_hu_18c15fdca4c025cd.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614015736570_hu_ec110903f60e0cb9.png 1024w" loading=lazy alt=image-20250614015736570 class=gallery-image data-flex-grow=137 data-flex-basis=328px></p><p>梯度压缩比可以到很高，99.9%，没有1000x？索引开销、bias 偏置没有剪枝，<strong>偏置对残差训练很重要</strong>。</p><h5 id=powersgd-low-rank-gradient-compression>PowerSGD: Low-Rank Gradient Compression</h5><p><strong>问题：稀疏梯度，在 all-reduce 环节会变得越来密集</strong></p><p>采用固定稀疏模式，粗粒度稀疏。</p><p>用低秩分解，来固定稀疏模式，粗粒度稀疏。</p><h4 id=gradient-quantization>Gradient Quantization</h4><h5 id=1-bit-sgd>1-Bit SGD</h5><p>把梯度量化为 1 bit，零阈值，同时保留 delta 值作为残差，缓解误差（累积到阈值，直接加回）。</p><p>每一列都增加一个 fp32 的缩放因子</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022206527.png width=1340 height=550 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022206527_hu_33e3a6a0be2b784.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022206527_hu_6358c5de2067400f.png 1024w" loading=lazy alt=image-20250614022206527 class=gallery-image data-flex-grow=243 data-flex-basis=584px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022215979.png width=1369 height=654 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022215979_hu_13c50d7a58cf59b0.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022215979_hu_cfd70140f1589f40.png 1024w" loading=lazy alt=image-20250614022215979 class=gallery-image data-flex-grow=209 data-flex-basis=502px></p><h5 id=threshold-quantization>Threshold Quantization</h5><p>设置 tau，大于 tau 为 tau，小于 -tau 为 -tau，之间为 0</p><p>需要经验选择 tau 值，同样有累积误差的机制</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022458482.png width=1168 height=535 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022458482_hu_4772a4f99a7d86d2.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022458482_hu_526326e0ef267f5a.png 1024w" loading=lazy alt=image-20250614022458482 class=gallery-image data-flex-grow=218 data-flex-basis=523px></p><h5 id=terngrad>TernGrad</h5><p>量化 g_i / max(g) 为 0, 1, -1 ，以概率来随机量化，期望一致，不需要累积误差。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022659938.png width=1124 height=613 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022659938_hu_bce9817bfcff6245.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022659938_hu_2f40ce47452d16fa.png 1024w" loading=lazy alt=image-20250614022659938 class=gallery-image data-flex-grow=183 data-flex-basis=440px></p><h3 id=delayed-gradient-update-overcome-the-latency-bottleneck>Delayed gradient update: overcome the latency bottleneck</h3><h4 id=bandwidth-vs-latency>Bandwidth vs. Latency</h4><p>带宽容易提升，剪枝量化、硬件提升；</p><p>延迟由物理限制，被光速限制</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022852163.png width=1367 height=506 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022852163_hu_3bb4127c97dc7a1d.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022852163_hu_1fe97341a227314d.png 1024w" loading=lazy alt=image-20250614022852163 class=gallery-image data-flex-grow=270 data-flex-basis=648px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022909511.png width=1451 height=568 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022909511_hu_4640f795858f9bd1.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614022909511_hu_6320affbd71d958a.png 1024w" loading=lazy alt=image-20250614022909511 class=gallery-image data-flex-grow=255 data-flex-basis=613px></p><p>延迟高，同步延迟会变高。</p><p>Delayed Gradient Averaging</p><p>超过太多步是不行的。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614030644450.png width=953 height=443 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614030644450_hu_2288e141cd40551d.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614030644450_hu_3c7109830e44853.png 1024w" loading=lazy alt=image-20250614030644450 class=gallery-image data-flex-grow=215 data-flex-basis=516px></p><p>最新的减去当前的来补偿延迟，avg_g 已经有了自己节点的梯度。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614032017831.png width=1111 height=551 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614032017831_hu_942944bcbb0c93e0.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614032017831_hu_dbf4c203be69330c.png 1024w" loading=lazy alt=image-20250614032017831 class=gallery-image data-flex-grow=201 data-flex-basis=483px></p><h2 id=lec21-on-device-training-and-transfer-learning>Lec21 On-Device Training and Transfer Learning</h2><h3 id=deep-leakage-fram-gradients-gradient-is-not-safe-to-share>Deep leakage fram gradients, gradient is not safe to share</h3><p><strong>Federated learning 联邦学习</strong></p><p>FedAvg algorithm，只传送权重/梯度</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614132630692.png width=1221 height=554 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614132630692_hu_b1e5d6461b4cf9f3.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614132630692_hu_6f22ce8fe997746a.png 1024w" loading=lazy alt=image-20250614132630692 class=gallery-image data-flex-grow=220 data-flex-basis=528px></p><ul><li>Membership Inference，指出可以用梯度判断某个记录是否在批次中使用</li><li>Property Inference，指出可以用梯度判断有特定属性的样本是否在批次中</li></ul><p><strong>Deep Leakage Attack</strong></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614133052908.png width=1097 height=603 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614133052908_hu_328c024376223aa6.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614133052908_hu_2949f90be1a8a1d3.png 1024w" loading=lazy alt=image-20250614133052908 class=gallery-image data-flex-grow=181 data-flex-basis=436px></p><p>一张图片ok，一个批次多个图片，也是可以的，顺序可能不确定，但是内容可以</p><p><strong>防御策略</strong></p><ul><li><p>增加 Gaussian / laplacian noise，过小没有用，过大破坏模型</p></li><li><p>梯度压缩，剪枝比例到 70% 基本不泄露，保持性能</p><p>只有很少的梯度泄露，复原不出来。</p></li></ul><h3 id=memory-bottleneck-of-on-device-training>Memory bottleneck of on-device training</h3><p>训练的内存占用大，因为批次大、需要存储中间激活值</p><p>（checkpoint 来计算换空间）</p><ul><li>Last 只微调最后一层？准确率下降很多</li><li>BN + Last</li><li><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614134803215.png width=1182 height=670 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614134803215_hu_e286425f22b4e7bd.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614134803215_hu_a20f4090eb0dfc7a.png 1024w" loading=lazy alt=image-20250614134803215 class=gallery-image data-flex-grow=176 data-flex-basis=423px></li></ul><p>代价很大，效果不好</p><h3 id=tiny-tansfer-learning-tinytl>Tiny tansfer learning (TinyTL)</h3><p>反向传播更新权重需要激活值，bias偏置不需要激活值</p><p>只微调偏置，Bias + Last</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140601427.png width=759 height=589 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140601427_hu_f482238898fad1d.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140601427_hu_f444c9813e043ca.png 1024w" loading=lazy alt=image-20250614140601427 class=gallery-image data-flex-grow=128 data-flex-basis=309px></p><p>引入轻量分支</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140753094.png width=775 height=556 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140753094_hu_1deaf0e469bd6c28.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140753094_hu_a639449d8b91ed69.png 1024w" loading=lazy alt=image-20250614140753094 class=gallery-image data-flex-grow=139 data-flex-basis=334px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140826258.png width=1132 height=573 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140826258_hu_692e285a95884417.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614140826258_hu_e705b6160a8355e5.png 1024w" loading=lazy alt=image-20250614140826258 class=gallery-image data-flex-grow=197 data-flex-basis=474px></p><p>比剪枝激活值更有效</p><h3 id=sparse-back-propagation-sparsebp>Sparse back-propagation (SparseBP)</h3><p>从生物学出发的方法。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141009296.png width=1125 height=563 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141009296_hu_15d754af3e1b574a.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141009296_hu_f2119d7de2a19dfc.png 1024w" loading=lazy alt=image-20250614141009296 class=gallery-image data-flex-grow=199 data-flex-basis=479px></p><p>只更新一部分层（深度深的高级特征）</p><p>只更新一层中的一部分参数</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141140713.png width=1143 height=572 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141140713_hu_2b9e09a3a0ee3687.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141140713_hu_1d1c040630c990db.png 1024w" loading=lazy alt=image-20250614141140713 class=gallery-image data-flex-grow=199 data-flex-basis=479px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141154775.png width=1039 height=597 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141154775_hu_248ad6f73c8a8bb9.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141154775_hu_8a4130dbb924cc99.png 1024w" loading=lazy alt=image-20250614141154775 class=gallery-image data-flex-grow=174 data-flex-basis=417px></p><p>怎么选择？</p><p>起始分辨率高，后面通道数多</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141448171.png width=1203 height=570 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141448171_hu_b99f12adb0dfd007.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141448171_hu_d4a8c29f9e4d4719.png 1024w" loading=lazy alt=image-20250614141448171 class=gallery-image data-flex-grow=211 data-flex-basis=506px></p><p>contribution analysis 贡献分析</p><p>自动求解器，类似敏感度分析</p><p>只更新前面的层，acc 甚至变差。</p><p>发现重复的起伏，peak是点卷积，curve是深度卷积</p><p>更新比例。</p><p>用进化算法搜索。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141750320.png width=1211 height=587 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141750320_hu_d52f34269720fefe.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614141750320_hu_c916b45384a44bee.png 1024w" loading=lazy alt=image-20250614141750320 class=gallery-image data-flex-grow=206 data-flex-basis=495px></p><p>SparseBP 的输出会更长？待研究。</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614142713545.png width=1206 height=587 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614142713545_hu_6f1cbc894b75ade.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614142713545_hu_f2a5cca2ef243cb1.png 1024w" loading=lazy alt=image-20250614142713545 class=gallery-image data-flex-grow=205 data-flex-basis=493px></p><h3 id=quantized-training-with-quantization-aware-scaling-qas>Quantized training with quantization aware scaling (QAS)</h3><p>在 int8 下，梯度值过小</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614143244669.png width=1133 height=632 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614143244669_hu_e2bda51e1653d1c.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614143244669_hu_79cbc7cf5752b467.png 1024w" loading=lazy alt=image-20250614143244669 class=gallery-image data-flex-grow=179 data-flex-basis=430px></p><p>修正缩放因子</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614143847560.png width=1067 height=552 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614143847560_hu_af9551ebd47f5774.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614143847560_hu_f07a08ec16c1473a.png 1024w" loading=lazy alt=image-20250614143847560 class=gallery-image data-flex-grow=193 data-flex-basis=463px></p><h3 id=pockengine-system-support-for-sparse-back-propagation>PockEngine: system support for sparse back-propagation</h3><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614144507235.png width=1001 height=573 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614144507235_hu_b5338a28ffee1a9d.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614144507235_hu_204a9f7a09eb3970.png 1024w" loading=lazy alt=image-20250614144507235 class=gallery-image data-flex-grow=174 data-flex-basis=419px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614144519082.png width=970 height=560 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614144519082_hu_dee383ff135c8fb6.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614144519082_hu_46582d5442e6262f.png 1024w" loading=lazy alt=image-20250614144519082 class=gallery-image data-flex-grow=173 data-flex-basis=415px></p><p>多种芯片，编译中，运行轻，训练优化。</p><h2 id=lec22-quantum-machine-learning-1>Lec22 Quantum Machine Learning 1</h2><p>解码量子纠错代码</p><p>Noisy Intermediate-Scale Quantum (NISQ)</p><h3 id=single-qubit-state-and-gates>Single qubit state and gates</h3><h4 id=single-qubit-state>Single qubit state</h4><p>basic component => Quantum Bit (Qubit)</p><p>state => statevector</p><p>Bra-ket notation 狄拉克符号</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614151944103.png width=1649 height=1026 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614151944103_hu_dd25d8d08932184e.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614151944103_hu_84041915a2d090e1.png 1024w" loading=lazy alt=image-20250614151944103 class=gallery-image data-flex-grow=160 data-flex-basis=385px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614152018133.png width=1292 height=339 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614152018133_hu_63e0611035847d94.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614152018133_hu_1f5057980b11ebe2.png 1024w" loading=lazy alt=image-20250614152018133 class=gallery-image data-flex-grow=381 data-flex-basis=914px></p><p><strong>Measurement</strong></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614155838740.png width=1164 height=699 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614155838740_hu_bbe48f13240050ab.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614155838740_hu_3e22d8bc7c56dc40.png 1024w" loading=lazy alt=image-20250614155838740 class=gallery-image data-flex-grow=166 data-flex-basis=399px></p><p><strong>Bloch Sphere</strong> 布洛赫球</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614161304846.png width=1477 height=868 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614161304846_hu_c76ef274ede36861.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614161304846_hu_44ca0c3d16f48814.png 1024w" loading=lazy alt=image-20250614161304846 class=gallery-image data-flex-grow=170 data-flex-basis=408px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614161314120.png width=2553 height=1107 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614161314120_hu_5fce267d376fd511.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614161314120_hu_21c5be2e0c0f458.png 1024w" loading=lazy alt=image-20250614161314120 class=gallery-image data-flex-grow=230 data-flex-basis=553px></p><p>用布洛赫球去表示任意量子比特的状态</p><h4 id=single-qubit-gates>Single Qubit Gates</h4><p>所有 Quantum gates 量子门 都是 <strong>reversible</strong> 可逆的（保证能量是一致的）</p><p>最简单的量子门是恒等映射</p><p>可逆门可用矩阵、布洛赫球的旋转</p><h5 id=pauli-gates>Pauli Gates</h5><ul><li>X Gate (Not Gate)</li><li>Y Gate</li><li>Z Gate，0 => 0, 1 => -1, 全局相位 phase，常规无法测量，所以也认为一致</li></ul><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162343499.png width=2275 height=1234 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162343499_hu_29583f4275b6234a.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162343499_hu_f0e508a4a4646403.png 1024w" loading=lazy alt=image-20250614162343499 class=gallery-image data-flex-grow=184 data-flex-basis=442px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162450413.png width=1957 height=690 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162450413_hu_353b8b0ada3b0b7c.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162450413_hu_a6418387af0a0377.png 1024w" loading=lazy alt=image-20250614162450413 class=gallery-image data-flex-grow=283 data-flex-basis=680px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162815210.png width=2177 height=1017 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162815210_hu_53da467a68d7450c.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162815210_hu_813972b1d564ba8e.png 1024w" loading=lazy alt=image-20250614162815210 class=gallery-image data-flex-grow=214 data-flex-basis=513px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162859531.png width=1208 height=1111 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162859531_hu_9eca65bb6f93ce87.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614162859531_hu_6e0c156bf3b162a4.png 1024w" loading=lazy alt=image-20250614162859531 class=gallery-image data-flex-grow=108 data-flex-basis=260px></p><h5 id=hadamard-gate>Hadamard Gate</h5><p>创建叠加态</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163314899.png width=1715 height=758 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163314899_hu_283ab58aa44ef562.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163314899_hu_7f801f4a9137f4d2.png 1024w" loading=lazy alt=image-20250614163314899 class=gallery-image data-flex-grow=226 data-flex-basis=543px></p><h5 id=other-gates>Other Gates</h5><ul><li>Phase Gate</li><li>S Gate</li><li>S dagger Gate</li></ul><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163355276.png width=2038 height=792 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163355276_hu_7798c972f9b61006.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163355276_hu_bff7d08179e8e78e.png 1024w" loading=lazy alt=image-20250614163355276 class=gallery-image data-flex-grow=257 data-flex-basis=617px></p><h5 id=u-gate>U Gate</h5><p>可以表示所有，通用门</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163501587.png width=2187 height=977 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163501587_hu_cf17d9592b31f557.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163501587_hu_2ef1e5d38080774d.png 1024w" loading=lazy alt=image-20250614163501587 class=gallery-image data-flex-grow=223 data-flex-basis=537px></p><h3 id=multiple-qubit-state-and-gates>Multiple-qubit state and gates</h3><h4 id=multiple-qubit-state>Multiple-qubit state</h4><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163625648.png width=1570 height=503 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163625648_hu_841b40b9660289d3.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163625648_hu_220f98e5c5af51e3.png 1024w" loading=lazy alt=image-20250614163625648 class=gallery-image data-flex-grow=312 data-flex-basis=749px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163657750.png width=1986 height=899 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163657750_hu_3c5acd4722ef9ea2.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614163657750_hu_33258c54323b915e.png 1024w" loading=lazy alt=image-20250614163657750 class=gallery-image data-flex-grow=220 data-flex-basis=530px></p><h4 id=multiple-qubit-gates>Multiple-qubit gates</h4><ul><li>CNOT Gate</li><li>&mldr;</li></ul><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614164219673.png width=2430 height=885 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614164219673_hu_f39c7d4692689050.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614164219673_hu_6b1ba0dec140248b.png 1024w" loading=lazy alt=image-20250614164219673 class=gallery-image data-flex-grow=274 data-flex-basis=658px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614164713691.png width=1605 height=1186 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614164713691_hu_d4ce106d9ee4f09e.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614164713691_hu_bc4d302a03ad0380.png 1024w" loading=lazy alt=image-20250614164713691 class=gallery-image data-flex-grow=135 data-flex-basis=324px></p><h3 id=quantum-circuit>quantum circuit</h3><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165152849.png width=1932 height=1146 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165152849_hu_f4f5aa20384ed9c6.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165152849_hu_b57f8491b6e11bb2.png 1024w" loading=lazy alt=image-20250614165152849 class=gallery-image data-flex-grow=168 data-flex-basis=404px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165242605.png width=1972 height=1115 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165242605_hu_a809d5c4f20c8409.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165242605_hu_5df4f0515ad07288.png 1024w" loading=lazy alt=image-20250614165242605 class=gallery-image data-flex-grow=176 data-flex-basis=424px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165405669.png width=2437 height=896 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165405669_hu_5b4e48fdca2aa068.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165405669_hu_30138f6bd9f96ca.png 1024w" loading=lazy alt=image-20250614165405669 class=gallery-image data-flex-grow=271 data-flex-basis=652px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165644261.png width=2269 height=1056 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165644261_hu_6fce716f25dd73dc.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614165644261_hu_7267238d3c3346fc.png 1024w" loading=lazy alt=image-20250614165644261 class=gallery-image data-flex-grow=214 data-flex-basis=515px></p><p>数据编码/上传代价是现在的主要瓶颈。</p><h3 id=the-nisq-era-and-compilation-problems>the NISQ era and compilation problems</h3><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614170857415.png width=2148 height=350 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614170857415_hu_860c41ab590d64e5.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614170857415_hu_366c7a296811f707.png 1024w" loading=lazy alt=image-20250614170857415 class=gallery-image data-flex-grow=613 data-flex-basis=1472px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614170705556.png width=1414 height=205 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614170705556_hu_5116d888174b1bca.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614170705556_hu_a59627445ed66766.png 1024w" loading=lazy alt=image-20250614170705556 class=gallery-image data-flex-grow=689 data-flex-basis=1655px></p><p>Single-qubit X error rate => 1.718e-3</p><p>CNOT error rate => 6.973e-2</p><p>不同量子比特的性能可能不同，误差率。</p><p>Sabre Qubit Mapping</p><p>看交换后能执行的门，启发式交换</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614172900090.png width=2487 height=1313 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614172900090_hu_eb07847247d66a86.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614172900090_hu_60504c984c87a341.png 1024w" loading=lazy alt=image-20250614172900090 class=gallery-image data-flex-grow=189 data-flex-basis=454px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614172953543.png width=2473 height=791 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614172953543_hu_9baf264039b80cd8.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614172953543_hu_c8551bffde8ff7a9.png 1024w" loading=lazy alt=image-20250614172953543 class=gallery-image data-flex-grow=312 data-flex-basis=750px></p><p><strong>QuantumNAS</strong></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173036947.png width=2313 height=991 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173036947_hu_97381e94e42697e8.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173036947_hu_619fff6cc9af196d.png 1024w" loading=lazy alt=image-20250614173036947 class=gallery-image data-flex-grow=233 data-flex-basis=560px></p><h3 id=the-example-workflow-and-compiler-on-neutral-atom-quantum-computer>the example workflow and compiler on neutral atom quantum computer</h3><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173159070.png width=2129 height=1329 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173159070_hu_cd6beb395e263fac.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173159070_hu_b97073934485b825.png 1024w" loading=lazy alt=image-20250614173159070 class=gallery-image data-flex-grow=160 data-flex-basis=384px></p><p>最大 k 割去优化编译</p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173537427.png width=2424 height=1145 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173537427_hu_7371898682497bae.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614173537427_hu_3a511f608888dcfc.png 1024w" loading=lazy alt=image-20250614173537427 class=gallery-image data-flex-grow=211 data-flex-basis=508px></p><h2 id=lec23-quantum-machine-learning-2>Lec23 Quantum Machine Learning 2</h2><p><a class=link href="https://www.dropbox.com/scl/fi/wxpnpwkrl6pw7lb4n4vrg/Lec23-Quantum-ML-II.pdf?rlkey=21msd9zdilhry5pydlkvbn7n4&amp;e=1&amp;st=aoyc9pzv&amp;dl=0" target=_blank rel=noopener>Lec23-Quantum-ML-II.pdf</a>
<sup><svg aria-hidden="true" focusable="false" viewBox="0 0 100 100" width="12" height="12" class="icon outbound"><path fill="currentColor" d="M18.8 85.1h56c2.2.0 4-1.8 4-4v-32h-8v28h-48v-48h28v-8h-32c-2.2.0-4 1.8-4 4v56C14.8 83.3 16.6 85.1 18.8 85.1z"/><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"/></svg></sup></p><blockquote><p>TBD.</p></blockquote><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614174243356.png width=2159 height=1126 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614174243356_hu_55ecf486e0158fbb.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614174243356_hu_fc52d1d5faec2932.png 1024w" loading=lazy alt=image-20250614174243356 class=gallery-image data-flex-grow=191 data-flex-basis=460px></p><h3 id=parameterized-quantum-circuit-pqc>Parameterized Quantum Circuit (PQC)</h3><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614174538876.png width=2276 height=1318 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614174538876_hu_93e85cb972818325.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614174538876_hu_c2a8a39b3ec8fb5d.png 1024w" loading=lazy alt=image-20250614174538876 class=gallery-image data-flex-grow=172 data-flex-basis=414px></p><p><img src=/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614175013836.png width=2108 height=1306 srcset="/p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614175013836_hu_9a44d9bd794ff683.png 480w, /p/mit6.5940-tinyml-and-efficient-deep-learning-computing/note.assets/image-20250614175013836_hu_5cb80f51af00617a.png 1024w" loading=lazy alt=image-20250614175013836 class=gallery-image data-flex-grow=161 data-flex-basis=387px></p><p>硬件效率</p><h3 id=pqc-training>PQC Training</h3><h3 id=quantum-classifiers>Quantum Classifiers</h3><h3 id=noise-aware-on-chip-training-qoc-of-pqc>Noise Aware On-Chip Training (QOC) of PQC</h3><h3 id=torchquantum-library-for-qml>TorchQuantum Library for QML</h3><h3 id=robust-quantum-architecture-search>Robust Quantum Architecture Search</h3></section><footer class=article-footer><section class=article-tags><a href=/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/>学习笔记</a>
<a href=/tags/mlsys/>MLsys</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>最后更新于 2025 年 6 月 14 日 18:55 CST</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/cmu_15-445_database_2024fall_p1/><div class=article-image><img src=/p/cmu_15-445_database_2024fall_p1/cover.45ed0849550e8bf447f0cbab3eb6bf2f_hu_b14e6486dba5d61e.jpeg width=250 height=150 loading=lazy alt="Featured image of post 『学习笔记』CMU 15-445 (2024 fall) Project #1 - Buffer Pool Manager" data-key=CMU_15-445_database_2024fall_P1 data-hash="md5-Re0ISVUOi/RH8MurPra/Lw=="></div><div class=article-details><h2 class=article-title>『学习笔记』CMU 15-445 (2024 fall) Project #1 - Buffer Pool Manager</h2></div></a></article><article class=has-image><a href=/p/cmu_15-445_database_2024fall_hw1/><div class=article-image><img src=/p/cmu_15-445_database_2024fall_hw1/cover.6720ee12dcab4376e768ab60d9784dad_hu_43dc54f7d72b6d6c.jpeg width=250 height=150 loading=lazy alt="Featured image of post 『学习笔记』CMU 15-445 (2024 fall) Homework #1 - SQL" data-key=CMU_15-445_database_2024fall_HW1 data-hash="md5-ZyDuEtyrQ3bnaKtg2XhNrQ=="></div><div class=article-details><h2 class=article-title>『学习笔记』CMU 15-445 (2024 fall) Homework #1 - SQL</h2></div></a></article><article class=has-image><a href=/p/cmu_15-445_database_2024fall_p0/><div class=article-image><img src=/p/cmu_15-445_database_2024fall_p0/cover.c96c63a067f2239cda27d50e64c97e7e_hu_17adc0e0026cd6b9.jpeg width=250 height=150 loading=lazy alt="Featured image of post 『学习笔记』CMU 15-445 (2024 fall) Project #0 - C++ Primer" data-key=CMU_15-445_database_2024fall_P0 data-hash="md5-yWxjoGfyI5zaJ9UOZMl+fg=="></div><div class=article-details><h2 class=article-title>『学习笔记』CMU 15-445 (2024 fall) Project #0 - C++ Primer</h2></div></a></article><article class=has-image><a href=/p/cs149_2024_note/><div class=article-image><img src=/p/cs149_2024_note/cover.664e6dffed8260e16a7d442976d4323f_hu_9dada7ff6176f365.jpeg width=250 height=150 loading=lazy alt="Featured image of post 『学习笔记』CS149 (2024)" data-key=CS149_2024_note data-hash="md5-Zk5t/+2CYOFqfUQpdtQyPw=="></div><div class=article-details><h2 class=article-title>『学习笔记』CS149 (2024)</h2></div></a></article><article class=has-image><a href=/p/cs149_2024_asst5_writeup/><div class=article-image><img src=/p/cs149_2024_asst5_writeup/cover.6e334527582f43c21afa8b753f42b50c_hu_4c45e10e34ec4af0.jpeg width=250 height=150 loading=lazy alt="Featured image of post 『学习笔记』CS149 (2024): Assignment 5" data-key=CS149_2024_asst5_writeup data-hash="md5-bjNFJ1gvQ8Ia+ot1P0K1DA=="></div><div class=article-details><h2 class=article-title>『学习笔记』CS149 (2024): Assignment 5</h2></div></a></article></div></div></aside><script src=https://giscus.app/client.js data-repo=Livinfly/Livinfly.github.io data-repo-id=R_kgDON6qCKA data-category=Announcements data-category-id=DIC_kwDON6qCKM4CnBxR data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=zh-CN data-loading crossorigin=anonymous async></script><script>function setGiscusTheme(e){let t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:{setConfig:{theme:e}}},"https://giscus.app")}(function(){addEventListener("message",t=>{if(event.origin!=="https://giscus.app")return;e()}),window.addEventListener("onColorSchemeChange",e);function e(){setGiscusTheme(document.documentElement.dataset.scheme==="light"?"light":"dark")}})()</script><footer class=site-footer><section class=copyright>&copy;
2022 -
2025 Livinfly's Blog</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.31.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>